{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "tensor(2.5576, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "cost tensor(1.8494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(0.5091, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.1991, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "cost tensor(-0.9873, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(2.0470, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9959, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.1500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "cost tensor(-0.7559, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.9834, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9225, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "cost tensor(-0.5780, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.7576, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8612, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.0665, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "cost tensor(-0.6545, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.8337, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8873, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.0812, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "cost tensor(-0.9262, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(2.0314, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9760, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.0668, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "cost tensor(-0.7086, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.8704, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9050, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.1399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "cost tensor(-0.9600, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(2.1126, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9873, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.0121, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "cost tensor(-0.8379, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.9037, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9462, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.0703, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "cost tensor(-1.1353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(2.1615, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(1.0441, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.1360, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "cost tensor(-0.6384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.8897, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8847, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.3822, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "cost tensor(-0.4601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(2.0020, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8404, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.0942, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "cost tensor(-0.5170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.7673, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8439, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.1976, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "cost tensor(-0.8549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(2.0979, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9546, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.1791, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "cost tensor(-0.8901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(2.1038, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9654, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.1053, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "cost tensor(-0.8076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.9749, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9381, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.0783, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "cost tensor(-0.6819, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.8635, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8967, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.5300, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "cost tensor(-0.2535, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.9949, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.7885, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.2176, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "cost tensor(-0.5158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.8839, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8495, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.1870, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "cost tensor(-0.8704, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(2.0981, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9593, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.3130, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "cost tensor(-0.7457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(2.1354, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9232, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.2828, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "cost tensor(-0.3866, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.8563, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8131, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.1561, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "cost tensor(-0.9994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(2.1556, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9998, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.2854, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "cost tensor(-0.9498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(2.2505, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9847, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.2342, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "cost tensor(-0.5053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.8925, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8470, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(1.9834, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "cost tensor(-0.7486, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.8163, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9158, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.1243, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "cost tensor(-0.5855, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.8424, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8673, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.1075, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "cost tensor(-0.9541, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(2.0763, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9852, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.0292, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "cost tensor(-0.6630, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.8035, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8888, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.1190, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "cost tensor(-0.9430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(2.0803, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9817, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.2703, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "cost tensor(-0.5712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.9726, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8689, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.1831, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "cost tensor(-0.5865, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.8995, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8701, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.0381, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "cost tensor(-0.8209, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.9179, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9410, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.0568, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "cost tensor(-0.9149, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.9996, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9722, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(1.9683, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "cost tensor(-0.9465, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.9328, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9820, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.2963, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "cost tensor(-0.7909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(2.1506, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9366, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.1858, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "cost tensor(-0.5588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.8832, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8615, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.2801, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "cost tensor(-0.7492, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(2.1057, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9235, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.2933, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "cost tensor(-0.5668, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.9916, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8685, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.2632, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "cost tensor(-0.5344, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.9403, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8573, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.2574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "cost tensor(-0.7682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(2.0968, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9288, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.2448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "cost tensor(-0.5432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.9287, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8592, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.1608, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "cost tensor(-0.6041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.8902, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8748, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(1.9823, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "cost tensor(-0.8526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.8843, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9506, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.3704, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "cost tensor(-0.2737, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.8596, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.7845, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.0337, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "cost tensor(-0.8754, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.9502, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9589, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.2250, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "cost tensor(-0.4694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.8590, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8355, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.3025, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "cost tensor(-0.5624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.9974, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8675, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.3055, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "cost tensor(-0.7652, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(2.1418, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9290, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.0703, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "cost tensor(-0.9111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(2.0104, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9711, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.0616, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "cost tensor(-0.7084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.8653, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9048, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.0302, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "cost tensor(-0.6285, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.7813, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8774, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.1441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "cost tensor(-1.0146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(2.1541, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(1.0047, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.3477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "cost tensor(-0.5265, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(2.0157, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8586, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(1.9850, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "cost tensor(-0.7783, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.8376, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9257, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.1044, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "cost tensor(-1.0877, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(2.1639, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(1.0282, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.2160, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "cost tensor(-0.4686, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.8498, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.8348, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(1.9385, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "cost tensor(-0.9189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(1.8850, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.9724, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8d398c34975f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0mdata_tsne_rp2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;31m# need to call `.task_done()` because we don't use `.join()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "delta=5\n",
    "def gausin_distance(x,y,delta=delta):\n",
    "    H= torch.norm(x-y).cuda()\n",
    "    distance = torch.exp(-H/2/(delta**2)).cuda()\n",
    "    \n",
    "    return distance\n",
    "def gram_matrix(data,kernel=gausin_distance):\n",
    "    #num=data.shape[0]\n",
    "    #matrix=torch.zeros((num,num)).cuda()\n",
    "    #for i in range (0,num):\n",
    "    #   for j in range (0,num):\n",
    "    #        matrix[i][j]=kernel(data[i],data[j])\n",
    "    num=data.shape[0]\n",
    "    datav3=torch.mm(data,torch.transpose(data, 0, 1).cuda()).cuda()\n",
    "    datav1=torch.diag(datav3,0).cuda()\n",
    "    #print(datav1.shape)\n",
    "    #matrix=torch.sqrt(datav1+datav2-2*datav3)\n",
    "    buf1=((-2)*datav3+datav1).cuda()\n",
    "    buf2=torch.transpose(buf1, 0, 1).cuda()\n",
    "    buf3=(buf2+datav1).cuda()\n",
    "    #print(buf3)\n",
    "    matrix = torch.exp(-buf3/2/(delta**2)).cuda()\n",
    "    return matrix\n",
    "def emerinal_hsic(X,Y):\n",
    "    num=X.shape[0]\n",
    "    #print(num)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Kx=gram_matrix(X)\n",
    "    \n",
    "    Ky=gram_matrix(Y)\n",
    "    \n",
    "    H = torch.eye(num).cuda() - torch.ones((num,num), dtype = torch.float32).cuda() / num\n",
    "    #print(H)\n",
    "    hsic=1/(num-1)*torch.trace(torch.mm(torch.mm(torch.mm(Kx,H).cuda(),Ky).cuda(),H).cuda()).cuda()\n",
    "    \n",
    "    return hsic\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets\n",
    "from PIL import Image\n",
    "batch_size=256\n",
    "class MyMNIST(torchvision.datasets.MNIST):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None,download=False):\n",
    "        \n",
    "        super(MyMNIST,self).__init__(root, train, transform, target_transform,download)\n",
    "        #print(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "        #print(target) \n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "            \n",
    "        else:\n",
    "            target=torch.as_tensor(target)\n",
    "            target = F.one_hot(target,num_classes=10).cuda()\n",
    "\n",
    "        return img, target\n",
    "mnist_data=torchvision.datasets.MNIST(root='./MNIST/', train=True, transform=transforms.ToTensor(), target_transform=None, download=False)\n",
    "data_loader = torch.utils.data.DataLoader(mnist_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2)\n",
    "#D_in, H1,H2,H3,H4,H5,H6,H7, D_out = 784, 64,32,16,8,4,2,1,10\n",
    "layer_num=[784,64]\n",
    "costs=[]\n",
    "dtype=torch.float\n",
    "continue_flag=0\n",
    "PATH='./checkpoint_cnn_onelayer'\n",
    "w=[]\n",
    "z=[]\n",
    "rate_list=[]\n",
    "if continue_flag:\n",
    "    \n",
    "    checkpoint = torch.load(PATH)\n",
    "    \n",
    "    w=checkpoint['w']\n",
    "\n",
    "else:\n",
    "    for i in range(len(layer_num)-1):\n",
    "    \n",
    "        \n",
    "        m = torch.nn.Conv2d(1, 5, 3, stride=2).cuda()\n",
    "        w.append(m)\n",
    "\n",
    "epoches=20\n",
    "lamda=5\n",
    "learning_rate=1\n",
    "data_tsne_label=[]\n",
    "data_tsne_rp1=[]\n",
    "data_tsne_rp2=[]\n",
    "for i in range(epoches): \n",
    "    for i_batch, (img,target) in enumerate(data_loader):\n",
    "        z=[]\n",
    "        img=img.reshape(-1,1,28,28).cuda()\n",
    "        \n",
    "        target=torch.as_tensor(target).cuda()\n",
    "        num=target.shape[0]\n",
    "        #print(num)\n",
    "        target=target.reshape(num,1).cuda()\n",
    "        #target = F.one_hot(target,num_classes=10).cuda()\n",
    "        target=torch.zeros((num,10)).cuda().scatter_(1,target,1).cuda()\n",
    "        \n",
    "        target=torch.as_tensor(target,dtype=dtype).cuda()\n",
    "        \n",
    "        z.append(F.relu(F.max_pool2d(w[0](img), 2)).cuda())\n",
    "        for cnt in range(len(w)-1):\n",
    "            \n",
    "            z.append(torch.tanh(torch.mm(z[cnt],w[cnt+1]).cuda()).cuda())\n",
    "            \n",
    "        grad_list=[]\n",
    "        \n",
    "        for cnt in range(len(z)-1):\n",
    "\n",
    "            \n",
    "            cost=-emerinal_hsic(z[cnt],img)-lamda*emerinal_hsic(z[cnt],target)\n",
    "        #print(cost1)\n",
    "            cost.backward(retain_graph=True)\n",
    "            print('!')\n",
    "        #print(w1.cuda().grad)\n",
    "            with torch.no_grad():\n",
    "                buf=torch.zeros((w[cnt].shape)).cuda()\n",
    "                #print(buf.shape)\n",
    "                buf.copy_(w[cnt].grad)\n",
    "                \n",
    "                grad_list.append(buf)\n",
    "            #print('grad1',torch.norm(grad1))\n",
    "            #w2 -= learning_rate * w2.grad\n",
    "\n",
    "            # Manually zero the gradients after updating weights\n",
    "                w[cnt].grad.zero_()\n",
    "                #print(cnt,torch.norm(buf))\n",
    "            #w2.grad.zero_()\n",
    "        z[-1]=z[-1].view(img.shape[0],-1).cuda()\n",
    "        img=img.view(img.shape[0],-1).cuda()\n",
    "        entropy=emerinal_hsic(z[-1],img)\n",
    "        information=lamda*emerinal_hsic(z[-1],target)\n",
    "        rate=entropy/information\n",
    "        cost=-entropy-information\n",
    "        #+1*(entropy/information)\n",
    "        cost.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #w1 -= learning_rate * w1.grad\n",
    "            #print('grad2',torch.norm(w2.grad))\n",
    "            w[-1].weight -= learning_rate * w[-1].weight.grad\n",
    "            \n",
    "            for cnt in range(len(w)-1):\n",
    "                w[cnt] -= learning_rate * grad_list[cnt]\n",
    "        # Manually zero the gradients after updating weights\n",
    "            w[-1].weight.grad.zero_()\n",
    "            \n",
    "        #    data_tsne_rp2.append(z2.cpu().data.numpy())\n",
    "        #    data_tsne_rp1.append(z1.cpu().data.numpy())\n",
    "        #    data_tsne_label.append(target.argmax(dim=1, keepdim=True).cpu().data.numpy())\n",
    "        \n",
    "\n",
    "        if i_batch %100 ==0:\n",
    "            costs.append(cost)\n",
    "            print(rate)\n",
    "            rate_list.append(rate)\n",
    "            print(i_batch)\n",
    "            print('cost',cost)\n",
    "            print('entr',entropy)\n",
    "            print('inf',information)\n",
    "            data_tsne_label.append(target.argmax(dim=1, keepdim=True).cpu().data.numpy())\n",
    "            torch.save({\n",
    "            'w': w,\n",
    "            'z': z,\n",
    "            'data_tsne_label':target.argmax(dim=1, keepdim=True).cpu().data.numpy()\n",
    "            ,'costs':costs\n",
    "            #'data_tsne_rp2':data_tsne_rp2,\n",
    "            #'data_tsne_rp1':data_tsne_rp1\n",
    "                \n",
    "            }, PATH)\n",
    "            \n",
    "            data_tsne_label=[]\n",
    "            #data_tsne_rp1=[]\n",
    "            #data_tsne_rp2=[]\n",
    "\n",
    "            #for j,zz in enumerate(z2):\n",
    "                #print(j,zz)\n",
    "                #print(j,target[j])\n",
    "            \n",
    "plt.plot(rate_list)\n",
    "plt.ylabel('rate')\n",
    "plt.xlabel('epochs (per 1)')\n",
    "plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
