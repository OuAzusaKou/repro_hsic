{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "delta=5\n",
    "def gausin_distance(x,y,delta=delta):\n",
    "    H= torch.norm(x-y).cuda()\n",
    "    distance = torch.exp(-H/2/(delta**2)).cuda()\n",
    "    \n",
    "    return distance\n",
    "def gram_matrix(data,kernel=gausin_distance):\n",
    "    #num=data.shape[0]\n",
    "    #matrix=torch.zeros((num,num)).cuda()\n",
    "    #for i in range (0,num):\n",
    "    #   for j in range (0,num):\n",
    "    #        matrix[i][j]=kernel(data[i],data[j])\n",
    "    num=data.shape[0]\n",
    "    datav3=torch.mm(data,torch.transpose(data, 0, 1).cuda()).cuda()\n",
    "    datav1=torch.diag(datav3,0).cuda()\n",
    "    #print(datav1.shape)\n",
    "    #matrix=torch.sqrt(datav1+datav2-2*datav3)\n",
    "    buf1=((-2)*datav3+datav1).cuda()\n",
    "    buf2=torch.transpose(buf1, 0, 1).cuda()\n",
    "    buf3=(buf2+datav1).cuda()\n",
    "    #print(buf3)\n",
    "    matrix = torch.exp(-buf3/2/(delta**2)).cuda()\n",
    "    return matrix\n",
    "def emerinal_hsic(X,Y):\n",
    "    num=X.shape[0]\n",
    "    #print(num)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Kx=gram_matrix(X)\n",
    "    \n",
    "    Ky=gram_matrix(Y)\n",
    "    \n",
    "    H = torch.eye(num).cuda() - torch.ones((num,num), dtype = torch.float32).cuda() / num\n",
    "    #print(H)\n",
    "    hsic=1/(num-1)*torch.trace(torch.mm(torch.mm(torch.mm(Kx,H).cuda(),Ky).cuda(),H).cuda()).cuda()\n",
    "    \n",
    "    return hsic\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets\n",
    "from PIL import Image\n",
    "batch_size=256\n",
    "class MyMNIST(torchvision.datasets.MNIST):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None,download=False):\n",
    "        \n",
    "        super(MyMNIST,self).__init__(root, train, transform, target_transform,download)\n",
    "        #print(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "        #print(target) \n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "            \n",
    "        else:\n",
    "            target=torch.as_tensor(target)\n",
    "            target = F.one_hot(target,num_classes=10).cuda()\n",
    "\n",
    "        return img, target\n",
    "mnist_data=torchvision.datasets.MNIST(root='./MNIST/', train=True, transform=transforms.ToTensor(), target_transform=None, download=False)\n",
    "data_loader = torch.utils.data.DataLoader(mnist_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2)\n",
    "#D_in, H1,H2,H3,H4,H5,H6,H7, D_out = 784, 64,32,16,8,4,2,1,10\n",
    "layer_num=[784,64]\n",
    "costs=[]\n",
    "dtype=torch.float\n",
    "continue_flag=1\n",
    "PATH='./checkpoint_cnn_multilayer'\n",
    "w=[]\n",
    "z=[]\n",
    "rate_list=[]\n",
    "if continue_flag:\n",
    "    \n",
    "    checkpoint = torch.load(PATH)\n",
    "    \n",
    "    w=checkpoint['w']\n",
    "\n",
    "else:\n",
    "    for i in range(len(layer_num)-1):\n",
    "    \n",
    "        \n",
    "        m = torch.nn.Conv2d(1, 32, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(32, 64, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(64, 3, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(3, 1, 3, stride=1,padding=0).cuda()\n",
    "        w.append(m)\n",
    "\n",
    "\n",
    "epoches=20\n",
    "lamda=20\n",
    "learning_rate=0\n",
    "data_tsne_label=[]\n",
    "data_tsne_rp1=[]\n",
    "data_tsne_rp2=[]\n",
    "for i in range(epoches): \n",
    "    for i_batch, (img,target) in enumerate(data_loader):\n",
    "        z=[]\n",
    "        costs=[]\n",
    "        informs=[]\n",
    "        entropys=[]\n",
    "        img=img.reshape(-1,1,28,28).cuda()\n",
    "        \n",
    "        target=torch.as_tensor(target).cuda()\n",
    "        num=target.shape[0]\n",
    "        #print(num)\n",
    "        target=target.reshape(num,1).cuda()\n",
    "        #target = F.one_hot(target,num_classes=10).cuda()\n",
    "        target=torch.zeros((num,10)).cuda().scatter_(1,target,1).cuda()\n",
    "        \n",
    "        target=torch.as_tensor(target,dtype=dtype).cuda()\n",
    "        \n",
    "        z.append(F.relu(F.max_pool2d(w[0](img), 2)).cuda())\n",
    "        #print(z[0].shape)\n",
    "        #for cnt in range(len(w)-1):\n",
    "            \n",
    "        #   z.append(torch.tanh(torch.mm(z[cnt],w[cnt+1]).cuda()).cuda())\n",
    "        z.append(F.relu(F.max_pool2d(w[1](z[0]), 2)).cuda())\n",
    "       # print(z[1].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[2](z[1]), 2)).cuda())\n",
    "       # print(z[2].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[3](z[2]), 2)).cuda())\n",
    "       # print(z[3].shape)\n",
    "        #print(z[1].shape)\n",
    "        \n",
    "        grad_list=[]\n",
    "        \n",
    "        for cnt in range(len(z)-1):\n",
    "            information=-lamda*emerinal_hsic(z[cnt].view(img.shape[0],-1).cuda(),target)\n",
    "            entropy=emerinal_hsic(z[cnt].view(img.shape[0],-1).cuda(),img.view(-1,784).cuda())\n",
    "            #\n",
    "            #z[cnt-1].view(img.shape[0],-1).cuda()\n",
    "            cost=-entropy\n",
    "            informs.append(information)\n",
    "            entropys.append(entropy)\n",
    "            costs.append(cost)\n",
    "            \n",
    "        #print(cost1)\n",
    "        #    cost.backward(retain_graph=True)\n",
    "        #    print('!')\n",
    "        #print(w1.cuda().grad)\n",
    "        #   with torch.no_grad():\n",
    "        #        buf=torch.zeros((w[cnt].shape)).cuda()\n",
    "                #print(buf.shape)\n",
    "        #       buf.copy_(w[cnt].grad)\n",
    "                \n",
    "        #        grad_list.append(buf)\n",
    "            #print('grad1',torch.norm(grad1))\n",
    "            #w2 -= learning_rate * w2.grad\n",
    "\n",
    "            # Manually zero the gradients after updating weights\n",
    "        #        w[cnt].grad.zero_()\n",
    "                #print(cnt,torch.norm(buf))\n",
    "            #w2.grad.zero_()\n",
    "        z[-1]=z[-1].view(img.shape[0],-1).cuda()\n",
    "        #print(z[-1].shape)\n",
    "        img=img.view(img.shape[0],-1).cuda()\n",
    "        entropy=emerinal_hsic(z[-1],img)\n",
    "        #z[-2].view(img.shape[0],-1).cuda()\n",
    "        information=lamda*emerinal_hsic(z[-1],target)\n",
    "        rate=entropy/information\n",
    "        #cost=-entropy-information+0.01*(entropy/information)\n",
    "        cost=-entropy\n",
    "        costs.append(cost)\n",
    "        entropys.append(entropy)\n",
    "        informs.append(information)\n",
    "        cost.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #w1 -= learning_rate * w1.grad\n",
    "            #print('grad2',torch.norm(w2.grad))\n",
    "            w[-1].weight -= learning_rate * w[-1].weight.grad\n",
    "            \n",
    "            \n",
    "            w[0].weight -= learning_rate * w[0].weight.grad\n",
    "            w[1].weight -= learning_rate * w[1].weight.grad\n",
    "            w[2].weight -= learning_rate * w[2].weight.grad\n",
    "            \n",
    "        # Manually zero the gradients after updating weights\n",
    "            w[-1].weight.grad.zero_()\n",
    "            w[0].weight.grad.zero_()\n",
    "            w[1].weight.grad.zero_()\n",
    "            w[2].weight.grad.zero_()\n",
    "        #    data_tsne_rp2.append(z2.cpu().data.numpy())\n",
    "        #    data_tsne_rp1.append(z1.cpu().data.numpy())\n",
    "        #    data_tsne_label.append(target.argmax(dim=1, keepdim=True).cpu().data.numpy())\n",
    "        \n",
    "\n",
    "        if i_batch %100 ==0:\n",
    "            \n",
    "            print(rate)\n",
    "            rate_list.append(rate)\n",
    "            print(i_batch)\n",
    "            print('costs',costs)\n",
    "            print('entropys',entropys)\n",
    "            print('informs',informs)\n",
    "            print('entr',entropy)\n",
    "            print('inf',information)\n",
    "            data_tsne_label.append(target.argmax(dim=1, keepdim=True).cpu().data.numpy())\n",
    "            torch.save({\n",
    "            'w': w,\n",
    "            'z': z,\n",
    "            'data_tsne_label':target.argmax(dim=1, keepdim=True).cpu().data.numpy()\n",
    "            ,'costs':costs\n",
    "            #'data_tsne_rp2':data_tsne_rp2,\n",
    "            #'data_tsne_rp1':data_tsne_rp1\n",
    "                \n",
    "            }, PATH)\n",
    "            \n",
    "            data_tsne_label=[]\n",
    "            #data_tsne_rp1=[]\n",
    "            #data_tsne_rp2=[]\n",
    "\n",
    "            #for j,zz in enumerate(z2):\n",
    "                #print(j,zz)\n",
    "                #print(j,target[j])\n",
    "            \n",
    "plt.plot(rate_list)\n",
    "plt.ylabel('rate')\n",
    "plt.xlabel('epochs (per 1)')\n",
    "plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "try: from sklearn.manifold import TSNE; HAS_SK = True\n",
    "except: HAS_SK = False; print('Please install sklearn for layer visualization')\n",
    "def plot_with_labels(lowDWeights, labels):\n",
    "    plt.cla()\n",
    "    X, Y = lowDWeights[:, 0], lowDWeights[:, 1]\n",
    "    print(len(labels))\n",
    "    for x, y, s in zip(X, Y, labels):\n",
    "        c = cm.rainbow(int(255 * s / 9)); plt.text(x, y, s, backgroundcolor=c, fontsize=9)\n",
    "    plt.xlim(X.min(), X.max()); plt.ylim(Y.min(), Y.max()); plt.title('Visualize last layer'); plt.show(); plt.pause(0.01)\n",
    "\n",
    "plt.ion()\n",
    "PATH='./checkpoint_cnn_multilayer2'\n",
    "checkpoint = torch.load(PATH)\n",
    "data_tsne_label=np.array(checkpoint['data_tsne_label'])\n",
    "data_tsne_rp=np.array(checkpoint['z'][3].view(data_tsne_label.shape[0],-1).cpu().data.numpy())\n",
    "#print(data_tsne_rp.shape)\n",
    "#print(data_tsne_rp[:50,:],data_tsne_label[:50])\n",
    "#for j in range(256):\n",
    "#    sums=0\n",
    "#    if data_tsne_label[j]==1:\n",
    "#        for i in range((data_tsne_rp.shape[1])):\n",
    "#            if data_tsne_rp[j][i]>=0:\n",
    "#                x=1\n",
    "#            else:\n",
    "#                x=0\n",
    "#            sums+=x*np.power(0.2,i)\n",
    "#        print(sums)\n",
    "#print()\n",
    "\n",
    "pic={'0':[],'1':[],'2':[],'3':[],'4':[],'5':[],'6':[],'7':[],'8':[],'9':[]}\n",
    "print(pic)\n",
    "for i in range(10):\n",
    "    print('!!!!')\n",
    "    for j in range(256):\n",
    "        if data_tsne_label[j]==i:\n",
    "           # print(i,str(i))\n",
    "           # pic[str(i)].append(data_tsne_rp[j])\n",
    "           # print(data_tsne_label[j])\n",
    "           print(data_tsne_rp[j][2])\n",
    "data_tsne_label=data_tsne_label.reshape((-1))\n",
    "plt.hist(data_tsne_rp[:256][2], bins=200, range=(0,100),histtype=\"stepfilled\", alpha=.8)\n",
    "\n",
    "#print(np.histogram(data_tsne_rp[:200],bins=100,range=(0,100)))\n",
    "#for i in range(10):\n",
    "    #plt.hist(pic[str(i)], bins=100, range=(0,100),normed=True,histtype=\"stepfilled\", alpha=.8)\n",
    "    #print('!!!')\n",
    "    #print(pic[str(i)])\n",
    "#print(data_tsne_label)\n",
    "#data_tsne=np.array(data_tsne[:][])\n",
    "'''\n",
    "if HAS_SK:\n",
    "    # Visualization of trained flatten layer (T-SNE)\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "    plot_only = 255\n",
    "    low_dim_embs = tsne.fit_transform(data_tsne_rp[:plot_only,:])\n",
    "    labels = data_tsne_label[:plot_only]\n",
    "    plot_with_labels(low_dim_embs, labels)\n",
    "'''\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "38\n",
      "43\n",
      "58\n",
      "65\n",
      "73\n",
      "84\n",
      "91\n",
      "94\n",
      "!!!!!\n",
      "tensor(1.3322, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.1745, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "entroys [tensor(1.0341, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "informs [tensor(1.3322, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "costs [tensor(-83.6704, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.8488, device='cuda:0', grad_fn=<NegBackward>), tensor(0.7382, device='cuda:0', grad_fn=<NegBackward>), tensor(0.9334, device='cuda:0', grad_fn=<NegBackward>), tensor(1.3322, device='cuda:0', grad_fn=<NegBackward>), tensor(15.4552, device='cuda:0', grad_fn=<NegBackward>), tensor(-17.6818, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "entr tensor(3.0852, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(17.6818, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "infsums [tensor(-85.0026, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "38\n",
      "43\n",
      "58\n",
      "65\n",
      "73\n",
      "84\n",
      "91\n",
      "94\n",
      "!!!!!\n",
      "tensor(1.3370, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "38\n",
      "58\n",
      "65\n",
      "73\n",
      "84\n",
      "91\n",
      "94\n",
      "139\n",
      "!!!!!\n",
      "tensor(1.4473, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "58\n",
      "65\n",
      "84\n",
      "91\n",
      "94\n",
      "!!!!!\n",
      "tensor(1.2490, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "38\n",
      "58\n",
      "65\n",
      "84\n",
      "91\n",
      "94\n",
      "139\n",
      "!!!!!\n",
      "tensor(1.4277, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5795c9de9cd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mcosts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "delta=5\n",
    "def gausin_distance(x,y,delta=delta):\n",
    "    H= torch.norm(x-y).cuda()\n",
    "    distance = torch.exp(-H/2/(delta**2)).cuda()\n",
    "    \n",
    "    return distance\n",
    "def gram_matrix(data,kernel=gausin_distance):\n",
    "    #num=data.shape[0]\n",
    "    #matrix=torch.zeros((num,num)).cuda()\n",
    "    #for i in range (0,num):\n",
    "    #   for j in range (0,num):\n",
    "    #        matrix[i][j]=kernel(data[i],data[j])\n",
    "    num=data.shape[0]\n",
    "    datav3=torch.mm(data,torch.transpose(data, 0, 1).cuda()).cuda()\n",
    "    datav1=torch.diag(datav3,0).cuda()\n",
    "    #print(datav1.shape)\n",
    "    #matrix=torch.sqrt(datav1+datav2-2*datav3)\n",
    "    buf1=((-2)*datav3+datav1).cuda()\n",
    "    buf2=torch.transpose(buf1, 0, 1).cuda()\n",
    "    buf3=(buf2+datav1).cuda()\n",
    "    #print(buf3)\n",
    "    matrix = torch.exp(-buf3/2/(delta**2)).cuda()\n",
    "    return matrix\n",
    "def emerinal_hsic(X,Y):\n",
    "    num=X.shape[0]\n",
    "    #print(num)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Kx=gram_matrix(X)\n",
    "    \n",
    "    Ky=gram_matrix(Y)\n",
    "    \n",
    "    H = torch.eye(num).cuda() - torch.ones((num,num), dtype = torch.float32).cuda() / num\n",
    "    #print(H)\n",
    "    hsic=1/(num-1)*torch.trace(torch.mm(torch.mm(torch.mm(Kx,H).cuda(),Ky).cuda(),H).cuda()).cuda()\n",
    "    \n",
    "    return hsic\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets\n",
    "from PIL import Image\n",
    "batch_size=256\n",
    "class MyMNIST(torchvision.datasets.MNIST):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None,download=False):\n",
    "        \n",
    "        super(MyMNIST,self).__init__(root, train, transform, target_transform,download)\n",
    "        #print(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "        #print(target) \n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "            \n",
    "        else:\n",
    "            target=torch.as_tensor(target)\n",
    "            target = F.one_hot(target,num_classes=10).cuda()\n",
    "\n",
    "        return img, target\n",
    "mnist_data=torchvision.datasets.MNIST(root='./MNIST/', train=True, transform=transforms.ToTensor(), target_transform=None, download=False)\n",
    "data_loader = torch.utils.data.DataLoader(mnist_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2)\n",
    "#D_in, H1,H2,H3,H4,H5,H6,H7, D_out = 784, 64,32,16,8,4,2,1,10\n",
    "layer_num=[784,64]\n",
    "costs=[]\n",
    "dtype=torch.float\n",
    "continue_flag=1\n",
    "PATH='./checkpoint_cnn_multilayer2'\n",
    "w=[]\n",
    "z=[]\n",
    "rate_list=[]\n",
    "if continue_flag:\n",
    "    \n",
    "    checkpoint = torch.load(PATH)\n",
    "    \n",
    "    w=checkpoint['w']\n",
    "\n",
    "else:\n",
    "    for i in range(len(layer_num)-1):\n",
    "    \n",
    "        \n",
    "        m = torch.nn.Conv2d(1, 32, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(32, 64, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(64, 32, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(32, 16, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(16, 3, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(3, 1, 2, stride=1,padding=0).cuda()\n",
    "        w.append(m)\n",
    "\n",
    "\n",
    "epoches=20\n",
    "lamda=20\n",
    "#5000\n",
    "learning_rate=0\n",
    "data_tsne_label=[]\n",
    "data_tsne_rp1=[]\n",
    "data_tsne_rp2=[]\n",
    "for i in range(epoches): \n",
    "    for i_batch, (img,target) in enumerate(data_loader):\n",
    "        z=[]\n",
    "        costs=[]\n",
    "        img=img.reshape(-1,1,28,28).cuda()\n",
    "        \n",
    "        target=torch.as_tensor(target).cuda()\n",
    "        num=target.shape[0]\n",
    "        #print(num)\n",
    "        target=target.reshape(num,1).cuda()\n",
    "        #target = F.one_hot(target,num_classes=10).cuda()\n",
    "        target=torch.zeros((num,10)).cuda().scatter_(1,target,1).cuda()\n",
    "        \n",
    "        target=torch.as_tensor(target,dtype=dtype).cuda()\n",
    "        \n",
    "        z.append(F.relu(F.max_pool2d(w[0](img), 2)).cuda())\n",
    "        #print(z[0].shape)\n",
    "        #for cnt in range(len(w)-1):\n",
    "            \n",
    "        #   z.append(torch.tanh(torch.mm(z[cnt],w[cnt+1]).cuda()).cuda())\n",
    "        z.append(F.relu(F.max_pool2d(w[1](z[0]), 2)).cuda())\n",
    "        #print(z[1].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[2](z[1]), 2)).cuda())\n",
    "        #print(z[2].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[3](z[2]), 2)).cuda())\n",
    "        #print(z[3].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[4](z[3]), 2)).cuda())\n",
    "        #print(z[4].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[5](z[4]), 2)).cuda())\n",
    "        #print(z[5].shape)\n",
    "        entropys=[]\n",
    "        informs=[]\n",
    "        infsums=[]\n",
    "        #print(z[1].shape)\n",
    "        h=z[3].view(img.shape[0],-1).cuda()\n",
    "        #print(h[:,1].shape)\n",
    "        information=lamda*emerinal_hsic(h.view(img.shape[0],-1).cuda(),target)\n",
    "        entropy=emerinal_hsic(h.view(img.shape[0],-1).cuda(),img.view(-1,784).cuda())\n",
    "        \n",
    "        \n",
    "        cost=0\n",
    "        number=0\n",
    "        for i in range(h.shape[1]):\n",
    "            cost=cost-lamda*emerinal_hsic(h[:,i].view(-1,1),target)\n",
    "            xnova=lamda*emerinal_hsic(h[:,i].view(-1,1),target)\n",
    "            if xnova > 3:\n",
    "                print(i)\n",
    "        print('!!!!!')\n",
    "        print(lamda*emerinal_hsic(h.view(256,-1),target))\n",
    "        infsum=cost\n",
    "        cost=cost+information\n",
    "        entropys.append(entropy)\n",
    "        informs.append(information)\n",
    "        infsums.append(infsum)\n",
    "        \n",
    "        costs.append(cost)\n",
    "        \n",
    "        grad_list=[]\n",
    "        \n",
    "        for cnt in range(len(z)-1):\n",
    "            information=-lamda*emerinal_hsic(z[cnt].view(img.shape[0],-1).cuda(),target)\n",
    "            entropy=emerinal_hsic(z[cnt].view(img.shape[0],-1).cuda(),img.view(-1,784).cuda())\n",
    "            cost=-information\n",
    "            costs.append(cost)\n",
    "        #print(cost1)\n",
    "        #    cost.backward(retain_graph=True)\n",
    "        #    print('!')\n",
    "        #print(w1.cuda().grad)\n",
    "        #   with torch.no_grad():\n",
    "        #        buf=torch.zeros((w[cnt].shape)).cuda()\n",
    "                #print(buf.shape)\n",
    "        #       buf.copy_(w[cnt].grad)\n",
    "                \n",
    "        #        grad_list.append(buf)\n",
    "            #print('grad1',torch.norm(grad1))\n",
    "            #w2 -= learning_rate * w2.grad\n",
    "\n",
    "            # Manually zero the gradients after updating weights\n",
    "        #        w[cnt].grad.zero_()\n",
    "                #print(cnt,torch.norm(buf))\n",
    "            #w2.grad.zero_()\n",
    "        z[-1]=z[-1].view(img.shape[0],-1).cuda()\n",
    "        #print(z[-1].shape)\n",
    "        img=img.view(img.shape[0],-1).cuda()\n",
    "        entropy=emerinal_hsic(z[-1],img)\n",
    "        information=lamda*emerinal_hsic(z[-1],target)\n",
    "        rate=entropy/information\n",
    "        cost=-information\n",
    "        costs.append(cost)\n",
    "        #+0.00005*(entropy/information)\n",
    "        cost.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #w1 -= learning_rate * w1.grad\n",
    "            #print('grad2',torch.norm(w2.grad))\n",
    "            w[-1].weight -= learning_rate * w[-1].weight.grad\n",
    "            \n",
    "            \n",
    "            w[0].weight -= learning_rate * w[0].weight.grad\n",
    "            w[1].weight -= learning_rate * w[1].weight.grad\n",
    "            w[2].weight -= learning_rate * w[2].weight.grad\n",
    "            w[3].weight -= learning_rate * w[3].weight.grad\n",
    "            w[4].weight -= learning_rate * w[4].weight.grad\n",
    "        # Manually zero the gradients after updating weights\n",
    "            w[-1].weight.grad.zero_()\n",
    "            w[0].weight.grad.zero_()\n",
    "            w[1].weight.grad.zero_()\n",
    "            w[2].weight.grad.zero_()\n",
    "            w[3].weight.grad.zero_()\n",
    "            w[4].weight.grad.zero_()\n",
    "        #    data_tsne_rp2.append(z2.cpu().data.numpy())\n",
    "        #    data_tsne_rp1.append(z1.cpu().data.numpy())\n",
    "        #    data_tsne_label.append(target.argmax(dim=1, keepdim=True).cpu().data.numpy())\n",
    "        \n",
    "\n",
    "        if i_batch %100 ==0:\n",
    "            #costs.append(cost)\n",
    "            print(rate)\n",
    "            rate_list.append(rate)\n",
    "            print(i_batch)\n",
    "            print('entroys',entropys)\n",
    "            print('informs',informs)\n",
    "            print('costs',costs)\n",
    "            print('entr',entropy)\n",
    "            print('inf',information)\n",
    "            print('infsums',infsums)\n",
    "            data_tsne_label.append(target.argmax(dim=1, keepdim=True).cpu().data.numpy())\n",
    "            torch.save({\n",
    "            'w': w,\n",
    "            'z': z,\n",
    "            'data_tsne_label':target.argmax(dim=1, keepdim=True).cpu().data.numpy()\n",
    "            ,'costs':costs\n",
    "            #'data_tsne_rp2':data_tsne_rp2,\n",
    "            #'data_tsne_rp1':data_tsne_rp1\n",
    "                \n",
    "            }, PATH)\n",
    "            \n",
    "            data_tsne_label=[]\n",
    "            #data_tsne_rp1=[]\n",
    "            #data_tsne_rp2=[]\n",
    "\n",
    "            #for j,zz in enumerate(z2):\n",
    "                #print(j,zz)\n",
    "                #print(j,target[j])\n",
    "            \n",
    "plt.plot(rate_list)\n",
    "plt.ylabel('rate')\n",
    "plt.xlabel('epochs (per 1)')\n",
    "plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
