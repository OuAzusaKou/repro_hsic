{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "delta=5\n",
    "def gausin_distance(x,y,delta=delta):\n",
    "    H= torch.norm(x-y).cuda()\n",
    "    distance = torch.exp(-H/2/(delta**2)).cuda()\n",
    "    \n",
    "    return distance\n",
    "def gram_matrix(data,kernel=gausin_distance):\n",
    "    #num=data.shape[0]\n",
    "    #matrix=torch.zeros((num,num)).cuda()\n",
    "    #for i in range (0,num):\n",
    "    #   for j in range (0,num):\n",
    "    #        matrix[i][j]=kernel(data[i],data[j])\n",
    "    num=data.shape[0]\n",
    "    datav3=torch.mm(data,torch.transpose(data, 0, 1).cuda()).cuda()\n",
    "    datav1=torch.diag(datav3,0).cuda()\n",
    "    #print(datav1.shape)\n",
    "    #matrix=torch.sqrt(datav1+datav2-2*datav3)\n",
    "    buf1=((-2)*datav3+datav1).cuda()\n",
    "    buf2=torch.transpose(buf1, 0, 1).cuda()\n",
    "    buf3=(buf2+datav1).cuda()\n",
    "    #print(buf3)\n",
    "    matrix = torch.exp(-buf3/2/(delta**2)).cuda()\n",
    "    return matrix\n",
    "def emerinal_hsic(X,Y):\n",
    "    num=X.shape[0]\n",
    "    #print(num)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Kx=gram_matrix(X)\n",
    "    \n",
    "    Ky=gram_matrix(Y)\n",
    "    \n",
    "    H = torch.eye(num).cuda() - torch.ones((num,num), dtype = torch.float32).cuda() / num\n",
    "    #print(H)\n",
    "    hsic=1/(num-1)*torch.trace(torch.mm(torch.mm(torch.mm(Kx,H).cuda(),Ky).cuda(),H).cuda()).cuda()\n",
    "    \n",
    "    return hsic\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets\n",
    "from PIL import Image\n",
    "batch_size=256\n",
    "class MyMNIST(torchvision.datasets.MNIST):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None,download=False):\n",
    "        \n",
    "        super(MyMNIST,self).__init__(root, train, transform, target_transform,download)\n",
    "        #print(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "        #print(target) \n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "            \n",
    "        else:\n",
    "            target=torch.as_tensor(target)\n",
    "            target = F.one_hot(target,num_classes=10).cuda()\n",
    "\n",
    "        return img, target\n",
    "mnist_data=torchvision.datasets.MNIST(root='./MNIST/', train=True, transform=transforms.ToTensor(), target_transform=None, download=False)\n",
    "data_loader = torch.utils.data.DataLoader(mnist_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2)\n",
    "#D_in, H1,H2,H3,H4,H5,H6,H7, D_out = 784, 64,32,16,8,4,2,1,10\n",
    "layer_num=[784,64]\n",
    "costs=[]\n",
    "dtype=torch.float\n",
    "continue_flag=1\n",
    "PATH='./checkpoint_cnn_nbpmultilayer'\n",
    "w=[]\n",
    "z=[]\n",
    "rate_list=[]\n",
    "reset_list=[]\n",
    "if continue_flag:\n",
    "    \n",
    "    checkpoint = torch.load(PATH)\n",
    "    \n",
    "    w=checkpoint['w']\n",
    "    \n",
    "    for k in reset_list:\n",
    "        w[k].reset_parameters()\n",
    "        \n",
    "\n",
    "else:\n",
    "    for i in range(len(layer_num)-1):\n",
    "    \n",
    "        \n",
    "        m = torch.nn.Conv2d(1, 32, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(32, 64, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(64, 3, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(3, 1, 3, stride=1,padding=0).cuda()\n",
    "        w.append(m)\n",
    "\n",
    "\n",
    "epoches=40\n",
    "lamda=20\n",
    "entropy_rate=0.1\n",
    "learning_rate=0.0005\n",
    "learning_ratel=0.000\n",
    "data_tsne_label=[]\n",
    "data_tsne_rp1=[]\n",
    "data_tsne_rp2=[]\n",
    "for i in range(epoches): \n",
    "    for i_batch, (img,target) in enumerate(data_loader):\n",
    "        z=[]\n",
    "        costs=[]\n",
    "        informs=[]\n",
    "        entropys=[]\n",
    "        img=img.reshape(-1,1,28,28).cuda()\n",
    "        \n",
    "        target=torch.as_tensor(target).cuda()\n",
    "        num=target.shape[0]\n",
    "        #print(num)\n",
    "        target=target.reshape(num,1).cuda()\n",
    "        #target = F.one_hot(target,num_classes=10).cuda()\n",
    "        target=torch.zeros((num,10)).cuda().scatter_(1,target,1).cuda()\n",
    "        \n",
    "        target=torch.as_tensor(target,dtype=dtype).cuda()\n",
    "        \n",
    "        z.append(F.relu(F.max_pool2d(w[0](img), 2)).cuda())\n",
    "        #print(z[0].shape)\n",
    "        #for cnt in range(len(w)-1):\n",
    "            \n",
    "        #   z.append(torch.tanh(torch.mm(z[cnt],w[cnt+1]).cuda()).cuda())\n",
    "        z.append(F.relu(F.max_pool2d(w[1](z[0]), 2)).cuda())\n",
    "        #print(z[1].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[2](z[1]), 2)).cuda())\n",
    "        #print(z[2].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[3](z[2]), 2)).cuda())\n",
    "        #print(z[3].shape)\n",
    "        #print(z[1].shape)\n",
    "        \n",
    "        grad_listw=[]\n",
    "        grad_listb=[]\n",
    "        h=z[2].view(img.shape[0],-1).cuda()\n",
    "        #print(h[:,1].shape)\n",
    "        information=lamda*emerinal_hsic(z[2].view(img.shape[0],-1).cuda(),target)\n",
    "        entropy=emerinal_hsic(z[2].view(img.shape[0],-1).cuda(),img.view(-1,784).cuda())\n",
    "        \n",
    "        cost=0\n",
    "        for i in range(h.shape[1]):\n",
    "            cost=cost-lamda*emerinal_hsic(h[:,i].view(-1,1),target)\n",
    "        cost=cost+entropy_rate*entropy\n",
    "        entropys.append(entropy)\n",
    "        informs.append(information)\n",
    "        costs.append(cost)\n",
    "        cost.backward(retain_graph=True)\n",
    "        lock_bplist=[0,1,2]\n",
    "        with torch.no_grad():\n",
    "            for cnt in lock_bplist:\n",
    "                bufw=torch.zeros((w[cnt].weight.shape)).cuda()\n",
    "                #print(buf.shape)\n",
    "                bufw.copy_(w[cnt].weight.grad)\n",
    "                \n",
    "                grad_listw.append(bufw)\n",
    "                \n",
    "                bufb=torch.zeros((w[cnt].bias.shape)).cuda()\n",
    "                #print(buf.shape)\n",
    "                bufb.copy_(w[cnt].bias.grad)\n",
    "                \n",
    "                grad_listb.append(bufb)\n",
    "            for nu in range(2+1):\n",
    "                w[nu].weight.grad.zero_()\n",
    "                w[nu].bias.grad.zero_()\n",
    "        \n",
    "        '''\n",
    "        for cnt in range(lock_list):\n",
    "            information=lamda*emerinal_hsic(z[cnt].view(img.shape[0],-1).cuda(),target)\n",
    "            entropy=emerinal_hsic(z[cnt].view(img.shape[0],-1).cuda(),img.view(-1,784).cuda())\n",
    "            cost=-entropy-information\n",
    "            entropys.append(entropy)\n",
    "            informs.append(information)\n",
    "            costs.append(cost)\n",
    "        #print(cost1)\n",
    "            cost.backward(retain_graph=True)\n",
    "        #    print('!')\n",
    "        #print(w1.cuda().grad)\n",
    "            with torch.no_grad():\n",
    "                bufw=torch.zeros((w[cnt].weight.shape)).cuda()\n",
    "                #print(buf.shape)\n",
    "                bufw.copy_(w[cnt].weight.grad)\n",
    "                \n",
    "                grad_listw.append(bufw)\n",
    "                \n",
    "                bufb=torch.zeros((w[cnt].bias.shape)).cuda()\n",
    "                #print(buf.shape)\n",
    "                bufb.copy_(w[cnt].bias.grad)\n",
    "                \n",
    "                grad_listb.append(bufb)\n",
    "            #print('grad1',torch.norm(grad1))\n",
    "            #w2 -= learning_rate * w2.grad\n",
    "\n",
    "            # Manually zero the gradients after updating weights\n",
    "                for nu in range(cnt+1):\n",
    "                    w[nu].weight.grad.zero_()\n",
    "                    w[nu].bias.grad.zero_()\n",
    "                #print(cnt,torch.norm(buf))\n",
    "            #w2.grad.zero_()\n",
    "        '''\n",
    "        z[-1]=z[-1].view(img.shape[0],-1).cuda()\n",
    "        #print(z[-1].shape)\n",
    "        img=img.view(img.shape[0],-1).cuda()\n",
    "        entropy=emerinal_hsic(z[-1],img.view(-1,784).cuda())\n",
    "        information=lamda*emerinal_hsic(z[-1],target)\n",
    "        rate=entropy/information\n",
    "        cost=-information\n",
    "        #+0.1*(entropy/information)\n",
    "        costs.append(cost)\n",
    "        informs.append(information)\n",
    "        entropys.append(entropy)\n",
    "        \n",
    "        \n",
    "        cost.backward()\n",
    "        lock_list=[]\n",
    "        with torch.no_grad():\n",
    "            #w1 -= learning_rate * w1.grad\n",
    "            #print('grad2',torch.norm(w2.grad))\n",
    "            w[-1].weight -= learning_ratel * w[-1].weight.grad\n",
    "            w[-1].bias-= learning_ratel * w[-1].bias.grad\n",
    "            w[-1].weight.grad.zero_()\n",
    "            w[-1].bias.grad.zero_()\n",
    "            for cnt in lock_list:\n",
    "                w[cnt].weight -= learning_rate * w[cnt].weight.grad\n",
    "                w[cnt].bias -= learning_rate * w[cnt].bias.grad\n",
    "            for cnt in range(len(z)-1):\n",
    "                w[cnt].weight.grad.zero_()\n",
    "                w[cnt].bias.grad.zero_()\n",
    "            for cnt in lock_bplist:\n",
    "                w[cnt].weight -= learning_rate * grad_listw[cnt]\n",
    "                w[cnt].bias -= learning_rate * grad_listb[cnt]\n",
    "            #w[0].weight -= learning_rate * w[0].weight.grad\n",
    "            #w[1].weight -= learning_rate * w[1].weight.grad\n",
    "            #w[2].weight -= learning_rate * w[2].weight.grad\n",
    "            \n",
    "        # Manually zero the gradients after updating weights\n",
    "            \n",
    "        #    data_tsne_rp2.append(z2.cpu().data.numpy())\n",
    "        #    data_tsne_rp1.append(z1.cpu().data.numpy())\n",
    "        #    data_tsne_label.append(target.argmax(dim=1, keepdim=True).cpu().data.numpy())\n",
    "        \n",
    "\n",
    "        if i_batch %100 ==0:\n",
    "            #costs.append(cost)\n",
    "            #print(rate)\n",
    "            rate_list.append(rate)\n",
    "            print(i_batch)\n",
    "            print('costs',costs)\n",
    "            print('entropys',entropys)\n",
    "            print('informs',informs)\n",
    "            print('entr',entropy)\n",
    "            print('inf',information)\n",
    "            data_tsne_label.append(target.argmax(dim=1, keepdim=True).cpu().data.numpy())\n",
    "            torch.save({\n",
    "            'w': w,\n",
    "            'z': z,\n",
    "            'data_tsne_label':target.argmax(dim=1, keepdim=True).cpu().data.numpy()\n",
    "            ,'costs':costs\n",
    "            #'data_tsne_rp2':data_tsne_rp2,\n",
    "            #'data_tsne_rp1':data_tsne_rp1\n",
    "                \n",
    "            }, PATH)\n",
    "            \n",
    "            data_tsne_label=[]\n",
    "            #data_tsne_rp1=[]\n",
    "            #data_tsne_rp2=[]\n",
    "\n",
    "            #for j,zz in enumerate(z2):\n",
    "                #print(j,zz)\n",
    "                #print(j,target[j])\n",
    "            \n",
    "plt.plot(rate_list)\n",
    "plt.ylabel('rate')\n",
    "plt.xlabel('epochs (per 1)')\n",
    "plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/matplotlib/text.py:1191: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "try: from sklearn.manifold import TSNE; HAS_SK = True\n",
    "except: HAS_SK = False; print('Please install sklearn for layer visualization')\n",
    "def plot_with_labels(lowDWeights, labels):\n",
    "    plt.cla()\n",
    "    X, Y = lowDWeights[:, 0], lowDWeights[:, 1]\n",
    "    print(len(labels))\n",
    "    for x, y, s in zip(X, Y, labels):\n",
    "        c = cm.rainbow(int(255 * s / 9)); plt.text(x, y, s, backgroundcolor=c, fontsize=9)\n",
    "    plt.xlim(X.min(), X.max()); plt.ylim(Y.min(), Y.max()); plt.title('Visualize last layer'); plt.show(); plt.pause(0.01)\n",
    "\n",
    "plt.ion()\n",
    "PATH='./checkpoint_cnn_nbpmultilayer2'\n",
    "checkpoint = torch.load(PATH)\n",
    "data_tsne_label=np.array(checkpoint['data_tsne_label'])\n",
    "data_tsne_rp=np.array(checkpoint['z'][-4].view(data_tsne_label.shape[0],-1).cpu().data.numpy())\n",
    "#print(data_tsne_rp.shape)\n",
    "#print(data_tsne_rp[:50,:],data_tsne_label[:50])\n",
    "#for j in range(256):\n",
    "#    sums=0\n",
    "#    if data_tsne_label[j]==1:\n",
    "#        for i in range((data_tsne_rp.shape[1])):\n",
    "#            if data_tsne_rp[j][i]>=0:\n",
    "#                x=1\n",
    "#            else:\n",
    "#                x=0\n",
    "#            sums+=x*np.power(0.2,i)\n",
    "#        print(sums)\n",
    "#print()\n",
    "'''\n",
    "pic={'0':[],'1':[],'2':[],'3':[],'4':[],'5':[],'6':[],'7':[],'8':[],'9':[]}\n",
    "print(pic)\n",
    "for i in range(10):\n",
    "    print('!!!!')\n",
    "    for j in range(256):\n",
    "        if data_tsne_label[j]==i:\n",
    "           # print(i,str(i))\n",
    "           # pic[str(i)].append(data_tsne_rp[j])\n",
    "           # print(data_tsne_label[j])\n",
    "           print(data_tsne_rp[j])\n",
    "data_tsne_label=data_tsne_label.reshape((-1))\n",
    "plt.hist(data_tsne_rp[:256], bins=200, range=(10,80),histtype=\"stepfilled\", alpha=.8)\n",
    "'''\n",
    "#print(np.histogram(data_tsne_rp[:200],bins=100,range=(0,100)))\n",
    "#for i in range(10):\n",
    "    #plt.hist(pic[str(i)], bins=100, range=(0,100),normed=True,histtype=\"stepfilled\", alpha=.8)\n",
    "    #print('!!!')\n",
    "    #print(pic[str(i)])\n",
    "#print(data_tsne_label)\n",
    "#data_tsne=np.array(data_tsne[:][])\n",
    "if HAS_SK:\n",
    "    # Visualization of trained flatten layer (T-SNE)\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "    plot_only = 255\n",
    "    low_dim_embs = tsne.fit_transform(data_tsne_rp[:plot_only,:])\n",
    "    labels = data_tsne_label[:plot_only]\n",
    "    fig = plt.gcf()\n",
    "    plot_with_labels(low_dim_embs, labels)\n",
    "    \n",
    "    \n",
    "    fig.savefig('./checkpoint_cnn_nbpmultilayer2_layer_-5.png')\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "delta=5\n",
    "def gausin_distance(x,y,delta=delta):\n",
    "    H= torch.norm(x-y).cuda()\n",
    "    distance = torch.exp(-H/2/(delta**2)).cuda()\n",
    "    \n",
    "    return distance\n",
    "def gram_matrix(data,kernel=gausin_distance):\n",
    "    #num=data.shape[0]\n",
    "    #matrix=torch.zeros((num,num)).cuda()\n",
    "    #for i in range (0,num):\n",
    "    #   for j in range (0,num):\n",
    "    #        matrix[i][j]=kernel(data[i],data[j])\n",
    "    num=data.shape[0]\n",
    "    datav3=torch.mm(data,torch.transpose(data, 0, 1).cuda()).cuda()\n",
    "    datav1=torch.diag(datav3,0).cuda()\n",
    "    #print(datav1.shape)\n",
    "    #matrix=torch.sqrt(datav1+datav2-2*datav3)\n",
    "    buf1=((-2)*datav3+datav1).cuda()\n",
    "    buf2=torch.transpose(buf1, 0, 1).cuda()\n",
    "    buf3=(buf2+datav1).cuda()\n",
    "    #print(buf3)\n",
    "    matrix = torch.exp(-buf3/2/(delta**2)).cuda()\n",
    "    return matrix\n",
    "def emerinal_hsic(X,Y):\n",
    "    num=X.shape[0]\n",
    "    #print(num)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Kx=gram_matrix(X)\n",
    "    \n",
    "    Ky=gram_matrix(Y)\n",
    "    \n",
    "    H = torch.eye(num).cuda() - torch.ones((num,num), dtype = torch.float32).cuda() / num\n",
    "    #print(H)\n",
    "    hsic=1/(num-1)*torch.trace(torch.mm(torch.mm(torch.mm(Kx,H).cuda(),Ky).cuda(),H).cuda()).cuda()\n",
    "    \n",
    "    return hsic\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets\n",
    "from PIL import Image\n",
    "batch_size=256\n",
    "class MyMNIST(torchvision.datasets.MNIST):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None,download=False):\n",
    "        \n",
    "        super(MyMNIST,self).__init__(root, train, transform, target_transform,download)\n",
    "        #print(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "        #print(target) \n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "            \n",
    "        else:\n",
    "            target=torch.as_tensor(target)\n",
    "            target = F.one_hot(target,num_classes=10).cuda()\n",
    "\n",
    "        return img, target\n",
    "mnist_data=torchvision.datasets.MNIST(root='./MNIST/', train=True, transform=transforms.ToTensor(), target_transform=None, download=False)\n",
    "data_loader = torch.utils.data.DataLoader(mnist_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2)\n",
    "#D_in, H1,H2,H3,H4,H5,H6,H7, D_out = 784, 64,32,16,8,4,2,1,10\n",
    "layer_num=[784,64]\n",
    "costs=[]\n",
    "dtype=torch.float\n",
    "continue_flag=1\n",
    "PATH='./checkpoint_cnn_nbpmultilayer2'\n",
    "w=[]\n",
    "z=[]\n",
    "rate_list=[]\n",
    "reset_list=[]\n",
    "if continue_flag:\n",
    "    \n",
    "    checkpoint = torch.load(PATH)\n",
    "    \n",
    "    w=checkpoint['w']\n",
    "    \n",
    "    for k in reset_list:\n",
    "        w[k].reset_parameters()\n",
    "        \n",
    "\n",
    "else:\n",
    "    for i in range(len(layer_num)-1):\n",
    "    \n",
    "        \n",
    "        m = torch.nn.Conv2d(1, 32, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(32, 64, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(64, 32, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(32, 16, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(16, 3, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(3, 1, 2, stride=1,padding=0).cuda()\n",
    "        w.append(m)\n",
    "\n",
    "\n",
    "epoches=40\n",
    "lamda=20\n",
    "inf_deprate=10\n",
    "entropy_rate=0.01\n",
    "learning_rate=0.00\n",
    "learning_ratel=0.000\n",
    "data_tsne_label=[]\n",
    "data_tsne_rp1=[]\n",
    "data_tsne_rp2=[]\n",
    "\n",
    "for i in range(epoches): \n",
    "    for i_batch, (img,target) in enumerate(data_loader):\n",
    "        z=[]\n",
    "        costs=[]\n",
    "        informs=[]\n",
    "        entropys=[]\n",
    "        infsums=[]\n",
    "        img=img.reshape(-1,1,28,28).cuda()\n",
    "        \n",
    "        target=torch.as_tensor(target).cuda()\n",
    "        num=target.shape[0]\n",
    "        #print(num)\n",
    "        target=target.reshape(num,1).cuda()\n",
    "        #target = F.one_hot(target,num_classes=10).cuda()\n",
    "        target=torch.zeros((num,10)).cuda().scatter_(1,target,1).cuda()\n",
    "        \n",
    "        target=torch.as_tensor(target,dtype=dtype).cuda()\n",
    "        \n",
    "        z.append(F.relu(F.max_pool2d(w[0](img), 2)).cuda())\n",
    "        #print(z[0].shape)\n",
    "        #for cnt in range(len(w)-1):\n",
    "            \n",
    "        #   z.append(torch.tanh(torch.mm(z[cnt],w[cnt+1]).cuda()).cuda())\n",
    "        z.append(F.relu(F.max_pool2d(w[1](z[0]), 2)).cuda())\n",
    "       # print(z[1].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[2](z[1]), 2)).cuda())\n",
    "       # print(z[2].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[3](z[2]), 2)).cuda())\n",
    "       # print(z[3].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[4](z[3]), 2)).cuda())\n",
    "       # print(z[4].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[5](z[4]), 2)).cuda())\n",
    "        #print(z[5].shape)\n",
    "        \n",
    "        \n",
    "        grad_listw=[]\n",
    "        grad_listb=[]\n",
    "        h=z[3].view(img.shape[0],-1).cuda()\n",
    "        post=linear(h)\n",
    "        #print(h[:,1].shape)\n",
    "        information=lamda*emerinal_hsic(h,target)\n",
    "        entropy=emerinal_hsic(h,img.view(-1,784).cuda())\n",
    "        back_inform=-lamda*emerinal_hsic(post.view(img.shape[0],-1).cuda(),target)\n",
    "        cost=0\n",
    "        for i in range(h.shape[1]):\n",
    "            cost=cost-lamda*emerinal_hsic(h[:,i].view(-1,1),target)\n",
    "            xnova=lamda*emerinal_hsic(h[:,i].view(-1,1),target)\n",
    "            #if xnova >1:\n",
    "                \n",
    "            print(xnova)\n",
    "        print('!!!')\n",
    "        \n",
    "        #print(lamda*emerinal_hsic(h[:,1].view(-1,1),h[:,3].view(-1,1)))\n",
    "        infsum=cost\n",
    "        infsums.append(infsum)\n",
    "        cost=cost+inf_deprate*(-cost-information)\n",
    "        entropys.append(entropy)\n",
    "        informs.append(information)\n",
    "        costs.append(cost)\n",
    "        cost.backward(retain_graph=True)\n",
    "        lock_bplist=[]\n",
    "        with torch.no_grad():\n",
    "            for cnt in lock_bplist:\n",
    "                bufw=torch.zeros((w[cnt].weight.shape)).cuda()\n",
    "                #print(buf.shape)\n",
    "                bufw.copy_(w[cnt].weight.grad)\n",
    "                \n",
    "                grad_listw.append(bufw)\n",
    "                \n",
    "                bufb=torch.zeros((w[cnt].bias.shape)).cuda()\n",
    "                #print(buf.shape)\n",
    "                bufb.copy_(w[cnt].bias.grad)\n",
    "                \n",
    "                grad_listb.append(bufb)\n",
    "            for nu in range(0):\n",
    "                w[nu].weight.grad.zero_()\n",
    "                w[nu].bias.grad.zero_()\n",
    "        \n",
    "        '''\n",
    "        for cnt in range(lock_list):\n",
    "            information=lamda*emerinal_hsic(z[cnt].view(img.shape[0],-1).cuda(),target)\n",
    "            entropy=emerinal_hsic(z[cnt].view(img.shape[0],-1).cuda(),img.view(-1,784).cuda())\n",
    "            cost=-entropy-information\n",
    "            entropys.append(entropy)\n",
    "            informs.append(information)\n",
    "            costs.append(cost)\n",
    "        #print(cost1)\n",
    "            cost.backward(retain_graph=True)\n",
    "        #    print('!')\n",
    "        #print(w1.cuda().grad)\n",
    "            with torch.no_grad():\n",
    "                bufw=torch.zeros((w[cnt].weight.shape)).cuda()\n",
    "                #print(buf.shape)\n",
    "                bufw.copy_(w[cnt].weight.grad)\n",
    "                \n",
    "                grad_listw.append(bufw)\n",
    "                \n",
    "                bufb=torch.zeros((w[cnt].bias.shape)).cuda()\n",
    "                #print(buf.shape)\n",
    "                bufb.copy_(w[cnt].bias.grad)\n",
    "                \n",
    "                grad_listb.append(bufb)\n",
    "            #print('grad1',torch.norm(grad1))\n",
    "            #w2 -= learning_rate * w2.grad\n",
    "\n",
    "            # Manually zero the gradients after updating weights\n",
    "                for nu in range(cnt+1):\n",
    "                    w[nu].weight.grad.zero_()\n",
    "                    w[nu].bias.grad.zero_()\n",
    "                #print(cnt,torch.norm(buf))\n",
    "            #w2.grad.zero_()\n",
    "        '''\n",
    "        z[-1]=z[-1].view(img.shape[0],-1).cuda()\n",
    "        #print(z[-1].shape)\n",
    "        img=img.view(img.shape[0],-1).cuda()\n",
    "        entropy=emerinal_hsic(z[-1],img.view(-1,784).cuda())\n",
    "        information=lamda*emerinal_hsic(z[-1],target)\n",
    "        rate=entropy/information\n",
    "        cost=-information+entropy\n",
    "        #+0.1*(entropy/information)\n",
    "        costs.append(cost)\n",
    "        informs.append(information)\n",
    "        entropys.append(entropy)\n",
    "        \n",
    "        back_inform.backward()\n",
    "        #cost.backward()\n",
    "        lock_list=[]\n",
    "        with torch.no_grad():\n",
    "            #w1 -= learning_rate * w1.grad\n",
    "            #print('grad2',torch.norm(w2.grad))\n",
    "            \n",
    "            w[-1].weight -= learning_ratel * w[-1].weight.grad\n",
    "            w[-1].bias-= learning_ratel * w[-1].bias.grad\n",
    "            w[-1].weight.grad.zero_()\n",
    "            w[-1].bias.grad.zero_()\n",
    "            for cnt in lock_list:\n",
    "                w[cnt].weight -= learning_rate * w[cnt].weight.grad\n",
    "                w[cnt].bias -= learning_rate * w[cnt].bias.grad\n",
    "            \n",
    "            for cnt in range(len(z)-1):\n",
    "                w[cnt].weight.grad.zero_()\n",
    "                w[cnt].bias.grad.zero_()\n",
    "            \n",
    "            for cnt in lock_bplist:\n",
    "                w[cnt].weight -= learning_rate * grad_listw[0]\n",
    "                w[cnt].bias -= learning_rate * grad_listb[0]\n",
    "                \n",
    "            #w[0].weight -= learning_rate * w[0].weight.grad\n",
    "            #w[1].weight -= learning_rate * w[1].weight.grad\n",
    "            #w[2].weight -= learning_rate * w[2].weight.grad\n",
    "            \n",
    "        # Manually zero the gradients after updating weights\n",
    "            \n",
    "        #    data_tsne_rp2.append(z2.cpu().data.numpy())\n",
    "        #    data_tsne_rp1.append(z1.cpu().data.numpy())\n",
    "        #    data_tsne_label.append(target.argmax(dim=1, keepdim=True).cpu().data.numpy())\n",
    "        \n",
    "\n",
    "        if i_batch %100 ==0:\n",
    "            #costs.append(cost)\n",
    "            #print(rate)\n",
    "            rate_list.append(rate)\n",
    "            print(i_batch)\n",
    "            print('costs',costs)\n",
    "            print('entropys',entropys)\n",
    "            print('informs',informs)\n",
    "            print('entr',entropy)\n",
    "            print('inf',information)\n",
    "            print('infsums',infsums)\n",
    "            data_tsne_label.append(target.argmax(dim=1, keepdim=True).cpu().data.numpy())\n",
    "            torch.save({\n",
    "            'w': w,\n",
    "            'z': z,\n",
    "            'data_tsne_label':target.argmax(dim=1, keepdim=True).cpu().data.numpy()\n",
    "            ,'costs':costs\n",
    "            #'data_tsne_rp2':data_tsne_rp2,\n",
    "            #'data_tsne_rp1':data_tsne_rp1\n",
    "                \n",
    "            }, PATH)\n",
    "            \n",
    "            data_tsne_label=[]\n",
    "            #data_tsne_rp1=[]\n",
    "            #data_tsne_rp2=[]\n",
    "\n",
    "            #for j,zz in enumerate(z2):\n",
    "                #print(j,zz)\n",
    "                #print(j,target[j])\n",
    "            \n",
    "plt.plot(rate_list)\n",
    "plt.ylabel('rate')\n",
    "plt.xlabel('epochs (per 1)')\n",
    "plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "delta=5\n",
    "def gausin_distance(x,y,delta=delta):\n",
    "    H= torch.norm(x-y).cuda()\n",
    "    distance = torch.exp(-H/2/(delta**2)).cuda()\n",
    "    \n",
    "    return distance\n",
    "def gram_matrix(data,kernel=gausin_distance):\n",
    "    #num=data.shape[0]\n",
    "    #matrix=torch.zeros((num,num)).cuda()\n",
    "    #for i in range (0,num):\n",
    "    #   for j in range (0,num):\n",
    "    #        matrix[i][j]=kernel(data[i],data[j])\n",
    "    num=data.shape[0]\n",
    "    datav3=torch.mm(data,torch.transpose(data, 0, 1).cuda()).cuda()\n",
    "    datav1=torch.diag(datav3,0).cuda()\n",
    "    #print(datav1.shape)\n",
    "    #matrix=torch.sqrt(datav1+datav2-2*datav3)\n",
    "    buf1=((-2)*datav3+datav1).cuda()\n",
    "    buf2=torch.transpose(buf1, 0, 1).cuda()\n",
    "    buf3=(buf2+datav1).cuda()\n",
    "    #print(buf3)\n",
    "    matrix = torch.exp(-buf3/2/(delta**2)).cuda()\n",
    "    return matrix\n",
    "def emerinal_hsic(X,Y):\n",
    "    num=X.shape[0]\n",
    "    #print(num)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Kx=gram_matrix(X)\n",
    "    \n",
    "    Ky=gram_matrix(Y)\n",
    "    \n",
    "    H = torch.eye(num).cuda() - torch.ones((num,num), dtype = torch.float32).cuda() / num\n",
    "    #print(H)\n",
    "    hsic=1/(num-1)*torch.trace(torch.mm(torch.mm(torch.mm(Kx,H).cuda(),Ky).cuda(),H).cuda()).cuda()\n",
    "    \n",
    "    return hsic\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets\n",
    "from PIL import Image\n",
    "batch_size=256\n",
    "class MyMNIST(torchvision.datasets.MNIST):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None,download=False):\n",
    "        \n",
    "        super(MyMNIST,self).__init__(root, train, transform, target_transform,download)\n",
    "        #print(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "        #print(target) \n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "            \n",
    "        else:\n",
    "            target=torch.as_tensor(target)\n",
    "            target = F.one_hot(target,num_classes=10).cuda()\n",
    "\n",
    "        return img, target\n",
    "mnist_data=torchvision.datasets.MNIST(root='./MNIST/', train=True, transform=transforms.ToTensor(), target_transform=None, download=False)\n",
    "data_loader = torch.utils.data.DataLoader(mnist_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2)\n",
    "#D_in, H1,H2,H3,H4,H5,H6,H7, D_out = 784, 64,32,16,8,4,2,1,10\n",
    "layer_num=[784,64]\n",
    "costs=[]\n",
    "dtype=torch.float\n",
    "continue_flag=1\n",
    "PATH='./checkpoint_cnn_nbpmultilayer3'\n",
    "w=[]\n",
    "z=[]\n",
    "rate_list=[]\n",
    "reset_list=[5]\n",
    "if continue_flag:\n",
    "    \n",
    "    checkpoint = torch.load(PATH)\n",
    "    \n",
    "    w=checkpoint['w']\n",
    "    \n",
    "    for k in reset_list:\n",
    "        w[k].reset_parameters()\n",
    "        \n",
    "\n",
    "else:\n",
    "    for i in range(len(layer_num)-1):\n",
    "    \n",
    "        \n",
    "        m = torch.nn.Conv2d(1, 32, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        \n",
    "        m=torch.nn.Conv2d(32, 64, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        #print(m.weight.shape)\n",
    "        m=torch.nn.Conv2d(64, 32, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(32, 16, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(16, 3, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(3, 1, 2, stride=1,padding=0).cuda()\n",
    "        w.append(m)\n",
    "\n",
    "\n",
    "epoches=40\n",
    "lamda=20\n",
    "entropy_rate=0.01\n",
    "learning_rate=0.000\n",
    "learning_ratel=0.001\n",
    "data_tsne_label=[]\n",
    "data_tsne_rp1=[]\n",
    "data_tsne_rp2=[]\n",
    "\n",
    "for i in range(epoches): \n",
    "    for i_batch, (img,target) in enumerate(data_loader):\n",
    "        z=[]\n",
    "        costs=[]\n",
    "        informs=[]\n",
    "        entropys=[]\n",
    "        img=img.reshape(-1,1,28,28).cuda()\n",
    "        \n",
    "        target=torch.as_tensor(target).cuda()\n",
    "        num=target.shape[0]\n",
    "        #print(num)\n",
    "        target=target.reshape(num,1).cuda()\n",
    "        #target = F.one_hot(target,num_classes=10).cuda()\n",
    "        target=torch.zeros((num,10)).cuda().scatter_(1,target,1).cuda()\n",
    "        \n",
    "        target=torch.as_tensor(target,dtype=dtype).cuda()\n",
    "        \n",
    "        z.append(F.relu(F.max_pool2d(w[0](img), 2)).cuda())\n",
    "        #print(z[0].shape)\n",
    "        #for cnt in range(len(w)-1):\n",
    "            \n",
    "        #   z.append(torch.tanh(torch.mm(z[cnt],w[cnt+1]).cuda()).cuda())\n",
    "        z.append(F.relu(F.max_pool2d(w[1](z[0]), 2)).cuda())\n",
    "       # print(z[1].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[2](z[1]), 2)).cuda())\n",
    "       # print(z[2].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[3](z[2]), 2)).cuda())\n",
    "       # print(z[3].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[4](z[3]), 2)).cuda())\n",
    "       # print(z[4].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[5](z[4]), 2)).cuda())\n",
    "        #print(z[5].shape)\n",
    "        \n",
    "        \n",
    "        grad_listw=[]\n",
    "        grad_listb=[]\n",
    "        lay=4\n",
    "        h=z[lay].view(img.shape[0],-1,z[lay].shape[-1]*z[lay].shape[-2]).cuda()\n",
    "        #print(h[:,1].shape)\n",
    "        information=lamda*emerinal_hsic(h.view(img.shape[0],-1),target)\n",
    "        entropy=emerinal_hsic(h.view(img.shape[0],-1),img.view(-1,784).cuda())\n",
    "        \n",
    "        cost=0\n",
    "        for i in range(h.shape[2]):\n",
    "            cost=cost-lamda*emerinal_hsic(h[:,:,i],target)\n",
    "        cost=cost+information\n",
    "        entropys.append(entropy)\n",
    "        informs.append(information)\n",
    "        costs.append(cost)\n",
    "        cost.backward(retain_graph=True)\n",
    "        lock_bplist=[4]\n",
    "        with torch.no_grad():\n",
    "            for cnt in lock_bplist:\n",
    "                bufw=torch.zeros((w[cnt].weight.shape)).cuda()\n",
    "                #print(buf.shape)\n",
    "                bufw.copy_(w[cnt].weight.grad)\n",
    "                \n",
    "                grad_listw.append(bufw)\n",
    "                \n",
    "                bufb=torch.zeros((w[cnt].bias.shape)).cuda()\n",
    "                #print(buf.shape)\n",
    "                bufb.copy_(w[cnt].bias.grad)\n",
    "                \n",
    "                grad_listb.append(bufb)\n",
    "            for nu in range(lay+1):\n",
    "                w[nu].weight.grad.zero_()\n",
    "                w[nu].bias.grad.zero_()\n",
    "        \n",
    "        '''\n",
    "        for cnt in range(lock_list):\n",
    "            information=lamda*emerinal_hsic(z[cnt].view(img.shape[0],-1).cuda(),target)\n",
    "            entropy=emerinal_hsic(z[cnt].view(img.shape[0],-1).cuda(),img.view(-1,784).cuda())\n",
    "            cost=-entropy-information\n",
    "            entropys.append(entropy)\n",
    "            informs.append(information)\n",
    "            costs.append(cost)\n",
    "        #print(cost1)\n",
    "            cost.backward(retain_graph=True)\n",
    "        #    print('!')\n",
    "        #print(w1.cuda().grad)\n",
    "            with torch.no_grad():\n",
    "                bufw=torch.zeros((w[cnt].weight.shape)).cuda()\n",
    "                #print(buf.shape)\n",
    "                bufw.copy_(w[cnt].weight.grad)\n",
    "                \n",
    "                grad_listw.append(bufw)\n",
    "                \n",
    "                bufb=torch.zeros((w[cnt].bias.shape)).cuda()\n",
    "                #print(buf.shape)\n",
    "                bufb.copy_(w[cnt].bias.grad)\n",
    "                \n",
    "                grad_listb.append(bufb)\n",
    "            #print('grad1',torch.norm(grad1))\n",
    "            #w2 -= learning_rate * w2.grad\n",
    "\n",
    "            # Manually zero the gradients after updating weights\n",
    "                for nu in range(cnt+1):\n",
    "                    w[nu].weight.grad.zero_()\n",
    "                    w[nu].bias.grad.zero_()\n",
    "                #print(cnt,torch.norm(buf))\n",
    "            #w2.grad.zero_()\n",
    "        '''\n",
    "        z[-1]=z[-1].view(img.shape[0],-1).cuda()\n",
    "        #print(z[-1].shape)\n",
    "        img=img.view(img.shape[0],-1).cuda()\n",
    "        entropy=emerinal_hsic(z[-1],img.view(-1,784).cuda())\n",
    "        information=lamda*emerinal_hsic(z[-1],target)\n",
    "        rate=entropy/information\n",
    "        cost=-information-entropy\n",
    "        #+0.1*(entropy/information)\n",
    "        costs.append(cost)\n",
    "        informs.append(information)\n",
    "        entropys.append(entropy)\n",
    "        \n",
    "        \n",
    "        cost.backward()\n",
    "        lock_list=[]\n",
    "        with torch.no_grad():\n",
    "            #w1 -= learning_rate * w1.grad\n",
    "            #print('grad2',torch.norm(w2.grad))\n",
    "            w[-1].weight -= learning_ratel * w[-1].weight.grad\n",
    "            w[-1].bias-= learning_ratel * w[-1].bias.grad\n",
    "            w[-1].weight.grad.zero_()\n",
    "            w[-1].bias.grad.zero_()\n",
    "            for cnt in lock_list:\n",
    "                w[cnt].weight -= learning_rate * w[cnt].weight.grad\n",
    "                w[cnt].bias -= learning_rate * w[cnt].bias.grad\n",
    "            for cnt in range(len(z)-1):\n",
    "                w[cnt].weight.grad.zero_()\n",
    "                w[cnt].bias.grad.zero_()\n",
    "            for cnt in lock_bplist:\n",
    "                w[cnt].weight -= learning_rate * grad_listw[0]\n",
    "                w[cnt].bias -= learning_rate * grad_listb[0]\n",
    "            #w[0].weight -= learning_rate * w[0].weight.grad\n",
    "            #w[1].weight -= learning_rate * w[1].weight.grad\n",
    "            #w[2].weight -= learning_rate * w[2].weight.grad\n",
    "            \n",
    "        # Manually zero the gradients after updating weights\n",
    "            \n",
    "        #    data_tsne_rp2.append(z2.cpu().data.numpy())\n",
    "        #    data_tsne_rp1.append(z1.cpu().data.numpy())\n",
    "        #    data_tsne_label.append(target.argmax(dim=1, keepdim=True).cpu().data.numpy())\n",
    "        \n",
    "\n",
    "        if i_batch %100 ==0:\n",
    "            #costs.append(cost)\n",
    "            #print(rate)\n",
    "            rate_list.append(rate)\n",
    "            print(i_batch)\n",
    "            print('costs',costs)\n",
    "            print('entropys',entropys)\n",
    "            print('informs',informs)\n",
    "            print('entr',entropy)\n",
    "            print('inf',information)\n",
    "            data_tsne_label.append(target.argmax(dim=1, keepdim=True).cpu().data.numpy())\n",
    "            torch.save({\n",
    "            'w': w,\n",
    "            'z': z,\n",
    "            'data_tsne_label':target.argmax(dim=1, keepdim=True).cpu().data.numpy()\n",
    "            ,'costs':costs\n",
    "            #'data_tsne_rp2':data_tsne_rp2,\n",
    "            #'data_tsne_rp1':data_tsne_rp1\n",
    "                \n",
    "            }, PATH)\n",
    "            \n",
    "            data_tsne_label=[]\n",
    "            #data_tsne_rp1=[]\n",
    "            #data_tsne_rp2=[]\n",
    "\n",
    "            #for j,zz in enumerate(z2):\n",
    "                #print(j,zz)\n",
    "                #print(j,target[j])\n",
    "            \n",
    "plt.plot(rate_list)\n",
    "plt.ylabel('rate')\n",
    "plt.xlabel('epochs (per 1)')\n",
    "plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
