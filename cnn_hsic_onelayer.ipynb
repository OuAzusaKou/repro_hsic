{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "torch.Size([256, 338])\n",
      "tensor(3.0950, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "cost tensor(2.9111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "entr tensor(0.1390, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(0.0449, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n",
      "torch.Size([256, 338])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7a92e9afbb58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0mdata_tsne_rp2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;31m# need to call `.task_done()` because we don't use `.join()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "delta=5\n",
    "def gausin_distance(x,y,delta=delta):\n",
    "    H= torch.norm(x-y).cuda()\n",
    "    distance = torch.exp(-H/2/(delta**2)).cuda()\n",
    "    \n",
    "    return distance\n",
    "def gram_matrix(data,kernel=gausin_distance):\n",
    "    #num=data.shape[0]\n",
    "    #matrix=torch.zeros((num,num)).cuda()\n",
    "    #for i in range (0,num):\n",
    "    #   for j in range (0,num):\n",
    "    #        matrix[i][j]=kernel(data[i],data[j])\n",
    "    num=data.shape[0]\n",
    "    datav3=torch.mm(data,torch.transpose(data, 0, 1).cuda()).cuda()\n",
    "    datav1=torch.diag(datav3,0).cuda()\n",
    "    #print(datav1.shape)\n",
    "    #matrix=torch.sqrt(datav1+datav2-2*datav3)\n",
    "    buf1=((-2)*datav3+datav1).cuda()\n",
    "    buf2=torch.transpose(buf1, 0, 1).cuda()\n",
    "    buf3=(buf2+datav1).cuda()\n",
    "    #print(buf3)\n",
    "    matrix = torch.exp(-buf3/2/(delta**2)).cuda()\n",
    "    return matrix\n",
    "def emerinal_hsic(X,Y):\n",
    "    num=X.shape[0]\n",
    "    #print(num)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Kx=gram_matrix(X)\n",
    "    \n",
    "    Ky=gram_matrix(Y)\n",
    "    \n",
    "    H = torch.eye(num).cuda() - torch.ones((num,num), dtype = torch.float32).cuda() / num\n",
    "    #print(H)\n",
    "    hsic=1/(num-1)*torch.trace(torch.mm(torch.mm(torch.mm(Kx,H).cuda(),Ky).cuda(),H).cuda()).cuda()\n",
    "    \n",
    "    return hsic\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets\n",
    "from PIL import Image\n",
    "batch_size=256\n",
    "class MyMNIST(torchvision.datasets.MNIST):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None,download=False):\n",
    "        \n",
    "        super(MyMNIST,self).__init__(root, train, transform, target_transform,download)\n",
    "        #print(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "        #print(target) \n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "            \n",
    "        else:\n",
    "            target=torch.as_tensor(target)\n",
    "            target = F.one_hot(target,num_classes=10).cuda()\n",
    "\n",
    "        return img, target\n",
    "mnist_data=torchvision.datasets.MNIST(root='./MNIST/', train=True, transform=transforms.ToTensor(), target_transform=None, download=False)\n",
    "data_loader = torch.utils.data.DataLoader(mnist_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2)\n",
    "#D_in, H1,H2,H3,H4,H5,H6,H7, D_out = 784, 64,32,16,8,4,2,1,10\n",
    "layer_num=[784,64]\n",
    "costs=[]\n",
    "dtype=torch.float\n",
    "continue_flag=0\n",
    "PATH='./checkpoint_cnn_onelayer'\n",
    "w=[]\n",
    "z=[]\n",
    "rate_list=[]\n",
    "if continue_flag:\n",
    "    \n",
    "    checkpoint = torch.load(PATH)\n",
    "    \n",
    "    w=checkpoint['w']\n",
    "\n",
    "else:\n",
    "    for i in range(len(layer_num)-1):\n",
    "        m = torch.nn.Conv2d(1, 2, 3, stride=1).cuda()\n",
    "        w.append(m)\n",
    "\n",
    "epoches=20\n",
    "lamda=5\n",
    "learning_rate=0.1\n",
    "data_tsne_label=[]\n",
    "data_tsne_rp1=[]\n",
    "data_tsne_rp2=[]\n",
    "for i in range(epoches): \n",
    "    for i_batch, (img,target) in enumerate(data_loader):\n",
    "        z=[]\n",
    "        img=img.reshape(-1,1,28,28).cuda()\n",
    "        \n",
    "        target=torch.as_tensor(target).cuda()\n",
    "        num=target.shape[0]\n",
    "        #print(num)\n",
    "        target=target.reshape(num,1).cuda()\n",
    "        #target = F.one_hot(target,num_classes=10).cuda()\n",
    "        target=torch.zeros((num,10)).cuda().scatter_(1,target,1).cuda()\n",
    "        \n",
    "        target=torch.as_tensor(target,dtype=dtype).cuda()\n",
    "        \n",
    "        z.append(F.relu(F.max_pool2d(w[0](img), 2)).cuda())\n",
    "        for cnt in range(len(w)-1):\n",
    "            \n",
    "            z.append(torch.tanh(torch.mm(z[cnt],w[cnt+1]).cuda()).cuda())\n",
    "            \n",
    "        grad_list=[]\n",
    "        \n",
    "        for cnt in range(len(z)-1):\n",
    "\n",
    "            \n",
    "            cost=-emerinal_hsic(z[cnt],img)-lamda*emerinal_hsic(z[cnt],target)\n",
    "        #print(cost1)\n",
    "            cost.backward(retain_graph=True)\n",
    "            print('!')\n",
    "        #print(w1.cuda().grad)\n",
    "            with torch.no_grad():\n",
    "                buf=torch.zeros((w[cnt].shape)).cuda()\n",
    "                #print(buf.shape)\n",
    "                buf.copy_(w[cnt].grad)\n",
    "                \n",
    "                grad_list.append(buf)\n",
    "            #print('grad1',torch.norm(grad1))\n",
    "            #w2 -= learning_rate * w2.grad\n",
    "\n",
    "            # Manually zero the gradients after updating weights\n",
    "                w[cnt].grad.zero_()\n",
    "                #print(cnt,torch.norm(buf))\n",
    "            #w2.grad.zero_()\n",
    "        z[-1]=z[-1].view(img.shape[0],-1).cuda()\n",
    "        print(z[-1].shape)\n",
    "        img=img.view(img.shape[0],-1).cuda()\n",
    "        #print(z[-1].shape)\n",
    "        entropy=emerinal_hsic(z[-1],img)\n",
    "        information=lamda*emerinal_hsic(z[-1],target)\n",
    "        rate=entropy/information\n",
    "        cost=-entropy-information+1*(entropy/information)\n",
    "        cost.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #w1 -= learning_rate * w1.grad\n",
    "            #print('grad2',torch.norm(w2.grad))\n",
    "            w[-1].weight -= learning_rate * w[-1].weight.grad\n",
    "            \n",
    "            for cnt in range(len(w)-1):\n",
    "                w[cnt] -= learning_rate * grad_list[cnt]\n",
    "        # Manually zero the gradients after updating weights\n",
    "            w[-1].weight.grad.zero_()\n",
    "            \n",
    "        #    data_tsne_rp2.append(z2.cpu().data.numpy())\n",
    "        #    data_tsne_rp1.append(z1.cpu().data.numpy())\n",
    "        #    data_tsne_label.append(target.argmax(dim=1, keepdim=True).cpu().data.numpy())\n",
    "        \n",
    "\n",
    "        if i_batch %100 ==0:\n",
    "            costs.append(cost)\n",
    "            print(rate)\n",
    "            rate_list.append(rate)\n",
    "            print(i_batch)\n",
    "            print('cost',cost)\n",
    "            print('entr',entropy)\n",
    "            print('inf',information)\n",
    "            data_tsne_label.append(target.argmax(dim=1, keepdim=True).cpu().data.numpy())\n",
    "            torch.save({\n",
    "            'w': w,\n",
    "            'z': z,\n",
    "            'data_tsne_label':target.argmax(dim=1, keepdim=True).cpu().data.numpy()\n",
    "            ,'costs':costs\n",
    "            #'data_tsne_rp2':data_tsne_rp2,\n",
    "            #'data_tsne_rp1':data_tsne_rp1\n",
    "                \n",
    "            }, PATH)\n",
    "            \n",
    "            data_tsne_label=[]\n",
    "            #data_tsne_rp1=[]\n",
    "            #data_tsne_rp2=[]\n",
    "\n",
    "            #for j,zz in enumerate(z2):\n",
    "                #print(j,zz)\n",
    "                #print(j,target[j])\n",
    "            \n",
    "plt.plot(rate_list)\n",
    "plt.ylabel('rate')\n",
    "plt.xlabel('epochs (per 1)')\n",
    "plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
