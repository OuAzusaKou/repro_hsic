{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "delta=5\n",
    "def gausin_distance(x,y,delta=delta):\n",
    "    H= torch.norm(x-y).cuda()\n",
    "    distance = torch.exp(-H/2/(delta**2)).cuda()\n",
    "    \n",
    "    return distance\n",
    "def gram_matrix(data,kernel=gausin_distance):\n",
    "    #num=data.shape[0]\n",
    "    #matrix=torch.zeros((num,num)).cuda()\n",
    "    #for i in range (0,num):\n",
    "    #   for j in range (0,num):\n",
    "    #        matrix[i][j]=kernel(data[i],data[j])\n",
    "    num=data.shape[0]\n",
    "    datav3=torch.mm(data,torch.transpose(data, 0, 1).cuda()).cuda()\n",
    "    datav1=torch.diag(datav3,0).cuda()\n",
    "    #print(datav1.shape)\n",
    "    #matrix=torch.sqrt(datav1+datav2-2*datav3)\n",
    "    buf1=((-2)*datav3+datav1).cuda()\n",
    "    buf2=torch.transpose(buf1, 0, 1).cuda()\n",
    "    buf3=(buf2+datav1).cuda()\n",
    "    #print(buf3)\n",
    "    matrix = torch.exp(-buf3/2/(delta**2)).cuda()\n",
    "    return matrix\n",
    "def emerinal_hsic(X,Y):\n",
    "    num=X.shape[0]\n",
    "    #print(num)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Kx=gram_matrix(X)\n",
    "    \n",
    "    Ky=gram_matrix(Y)\n",
    "    \n",
    "    H = torch.eye(num).cuda() - torch.ones((num,num), dtype = torch.float32).cuda() / num\n",
    "    #print(H)\n",
    "    hsic=1/(num-1)*torch.trace(torch.mm(torch.mm(torch.mm(Kx,H).cuda(),Ky).cuda(),H).cuda()).cuda()\n",
    "    \n",
    "    return hsic\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets\n",
    "from PIL import Image\n",
    "batch_size=256\n",
    "class MyMNIST(torchvision.datasets.MNIST):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None,download=False):\n",
    "        \n",
    "        super(MyMNIST,self).__init__(root, train, transform, target_transform,download)\n",
    "        #print(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "        #print(target) \n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "            \n",
    "        else:\n",
    "            target=torch.as_tensor(target)\n",
    "            target = F.one_hot(target,num_classes=10).cuda()\n",
    "\n",
    "        return img, target\n",
    "mnist_data=torchvision.datasets.MNIST(root='./MNIST/', train=True, transform=transforms.ToTensor(), target_transform=None, download=False)\n",
    "data_loader = torch.utils.data.DataLoader(mnist_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2)\n",
    "#D_in, H1,H2,H3,H4,H5,H6,H7, D_out = 784, 64,32,16,8,4,2,1,10\n",
    "layer_num=[784,64]\n",
    "costs=[]\n",
    "dtype=torch.float\n",
    "continue_flag=1\n",
    "PATH='./checkpoint_cnn_multilayer'\n",
    "w=[]\n",
    "z=[]\n",
    "rate_list=[]\n",
    "if continue_flag:\n",
    "    \n",
    "    checkpoint = torch.load(PATH)\n",
    "    \n",
    "    w=checkpoint['w']\n",
    "\n",
    "else:\n",
    "    for i in range(len(layer_num)-1):\n",
    "    \n",
    "        \n",
    "        m = torch.nn.Conv2d(1, 32, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(32, 64, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(64, 3, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(3, 1, 3, stride=1,padding=0).cuda()\n",
    "        w.append(m)\n",
    "\n",
    "\n",
    "epoches=20\n",
    "lamda=20\n",
    "learning_rate=0\n",
    "data_tsne_label=[]\n",
    "data_tsne_rp1=[]\n",
    "data_tsne_rp2=[]\n",
    "for i in range(epoches): \n",
    "    for i_batch, (img,target) in enumerate(data_loader):\n",
    "        z=[]\n",
    "        costs=[]\n",
    "        informs=[]\n",
    "        entropys=[]\n",
    "        img=img.reshape(-1,1,28,28).cuda()\n",
    "        \n",
    "        target=torch.as_tensor(target).cuda()\n",
    "        num=target.shape[0]\n",
    "        #print(num)\n",
    "        target=target.reshape(num,1).cuda()\n",
    "        #target = F.one_hot(target,num_classes=10).cuda()\n",
    "        target=torch.zeros((num,10)).cuda().scatter_(1,target,1).cuda()\n",
    "        \n",
    "        target=torch.as_tensor(target,dtype=dtype).cuda()\n",
    "        \n",
    "        z.append(F.relu(F.max_pool2d(w[0](img), 2)).cuda())\n",
    "        #print(z[0].shape)\n",
    "        #for cnt in range(len(w)-1):\n",
    "            \n",
    "        #   z.append(torch.tanh(torch.mm(z[cnt],w[cnt+1]).cuda()).cuda())\n",
    "        z.append(F.relu(F.max_pool2d(w[1](z[0]), 2)).cuda())\n",
    "       # print(z[1].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[2](z[1]), 2)).cuda())\n",
    "       # print(z[2].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[3](z[2]), 2)).cuda())\n",
    "       # print(z[3].shape)\n",
    "        #print(z[1].shape)\n",
    "        \n",
    "        grad_list=[]\n",
    "        \n",
    "        for cnt in range(len(z)-1):\n",
    "            information=-lamda*emerinal_hsic(z[cnt].view(img.shape[0],-1).cuda(),target)\n",
    "            entropy=emerinal_hsic(z[cnt].view(img.shape[0],-1).cuda(),img.view(-1,784).cuda())\n",
    "            #\n",
    "            #z[cnt-1].view(img.shape[0],-1).cuda()\n",
    "            cost=-entropy\n",
    "            informs.append(information)\n",
    "            entropys.append(entropy)\n",
    "            costs.append(cost)\n",
    "            \n",
    "        #print(cost1)\n",
    "        #    cost.backward(retain_graph=True)\n",
    "        #    print('!')\n",
    "        #print(w1.cuda().grad)\n",
    "        #   with torch.no_grad():\n",
    "        #        buf=torch.zeros((w[cnt].shape)).cuda()\n",
    "                #print(buf.shape)\n",
    "        #       buf.copy_(w[cnt].grad)\n",
    "                \n",
    "        #        grad_list.append(buf)\n",
    "            #print('grad1',torch.norm(grad1))\n",
    "            #w2 -= learning_rate * w2.grad\n",
    "\n",
    "            # Manually zero the gradients after updating weights\n",
    "        #        w[cnt].grad.zero_()\n",
    "                #print(cnt,torch.norm(buf))\n",
    "            #w2.grad.zero_()\n",
    "        z[-1]=z[-1].view(img.shape[0],-1).cuda()\n",
    "        #print(z[-1].shape)\n",
    "        img=img.view(img.shape[0],-1).cuda()\n",
    "        entropy=emerinal_hsic(z[-1],img)\n",
    "        #z[-2].view(img.shape[0],-1).cuda()\n",
    "        information=lamda*emerinal_hsic(z[-1],target)\n",
    "        rate=entropy/information\n",
    "        #cost=-entropy-information+0.01*(entropy/information)\n",
    "        cost=-entropy\n",
    "        costs.append(cost)\n",
    "        entropys.append(entropy)\n",
    "        informs.append(information)\n",
    "        cost.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #w1 -= learning_rate * w1.grad\n",
    "            #print('grad2',torch.norm(w2.grad))\n",
    "            w[-1].weight -= learning_rate * w[-1].weight.grad\n",
    "            \n",
    "            \n",
    "            w[0].weight -= learning_rate * w[0].weight.grad\n",
    "            w[1].weight -= learning_rate * w[1].weight.grad\n",
    "            w[2].weight -= learning_rate * w[2].weight.grad\n",
    "            \n",
    "        # Manually zero the gradients after updating weights\n",
    "            w[-1].weight.grad.zero_()\n",
    "            w[0].weight.grad.zero_()\n",
    "            w[1].weight.grad.zero_()\n",
    "            w[2].weight.grad.zero_()\n",
    "        #    data_tsne_rp2.append(z2.cpu().data.numpy())\n",
    "        #    data_tsne_rp1.append(z1.cpu().data.numpy())\n",
    "        #    data_tsne_label.append(target.argmax(dim=1, keepdim=True).cpu().data.numpy())\n",
    "        \n",
    "\n",
    "        if i_batch %100 ==0:\n",
    "            \n",
    "            print(rate)\n",
    "            rate_list.append(rate)\n",
    "            print(i_batch)\n",
    "            print('costs',costs)\n",
    "            print('entropys',entropys)\n",
    "            print('informs',informs)\n",
    "            print('entr',entropy)\n",
    "            print('inf',information)\n",
    "            data_tsne_label.append(target.argmax(dim=1, keepdim=True).cpu().data.numpy())\n",
    "            torch.save({\n",
    "            'w': w,\n",
    "            'z': z,\n",
    "            'data_tsne_label':target.argmax(dim=1, keepdim=True).cpu().data.numpy()\n",
    "            ,'costs':costs\n",
    "            #'data_tsne_rp2':data_tsne_rp2,\n",
    "            #'data_tsne_rp1':data_tsne_rp1\n",
    "                \n",
    "            }, PATH)\n",
    "            \n",
    "            data_tsne_label=[]\n",
    "            #data_tsne_rp1=[]\n",
    "            #data_tsne_rp2=[]\n",
    "\n",
    "            #for j,zz in enumerate(z2):\n",
    "                #print(j,zz)\n",
    "                #print(j,target[j])\n",
    "            \n",
    "plt.plot(rate_list)\n",
    "plt.ylabel('rate')\n",
    "plt.xlabel('epochs (per 1)')\n",
    "plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\npic={'0':[],'1':[],'2':[],'3':[],'4':[],'5':[],'6':[],'7':[],'8':[],'9':[]}\\nprint(pic)\\nfor i in range(10):\\n    print('!!!!')\\n    for j in range(256):\\n        if data_tsne_label[j]==i:\\n           # print(i,str(i))\\n           # pic[str(i)].append(data_tsne_rp[j])\\n           # print(data_tsne_label[j])\\n           print(data_tsne_rp[j])\\ndata_tsne_label=data_tsne_label.reshape((-1))\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "try: from sklearn.manifold import TSNE; HAS_SK = True\n",
    "except: HAS_SK = False; print('Please install sklearn for layer visualization')\n",
    "def plot_with_labels(lowDWeights, labels):\n",
    "    plt.cla()\n",
    "    \n",
    "    X, Y = lowDWeights[:, 0], lowDWeights[:, 1]\n",
    "    print(len(labels))\n",
    "    for x, y, s in zip(X, Y, labels):\n",
    "        c = cm.rainbow(int(255 * s / 9)); plt.text(x, y, s, backgroundcolor=c, fontsize=9)\n",
    "    \n",
    "    plt.xlim(X.min(), X.max()); plt.ylim(Y.min(), Y.max()); plt.title('Visualize last layer'); plt.show(); plt.pause(0.01)\n",
    "    \n",
    "plt.ion()\n",
    "PATH='./checkpoint_cnn_multilayer2'\n",
    "checkpoint = torch.load(PATH)\n",
    "data_tsne_label=np.array(checkpoint['data_tsne_label'])\n",
    "data_tsne_rp=np.array(checkpoint['z'][-4].view(data_tsne_label.shape[0],-1).cpu().data.numpy())\n",
    "#print(data_tsne_rp.shape)\n",
    "#print(data_tsne_rp[:50,:],data_tsne_label[:50])\n",
    "#for j in range(256):\n",
    "#    sums=0\n",
    "#    if data_tsne_label[j]==1:\n",
    "#        for i in range((data_tsne_rp.shape[1])):\n",
    "#            if data_tsne_rp[j][i]>=0:\n",
    "#                x=1\n",
    "#            else:\n",
    "#                x=0\n",
    "#            sums+=x*np.power(0.2,i)\n",
    "#        print(sums)\n",
    "#print()\n",
    "'''\n",
    "pic={'0':[],'1':[],'2':[],'3':[],'4':[],'5':[],'6':[],'7':[],'8':[],'9':[]}\n",
    "print(pic)\n",
    "for i in range(10):\n",
    "    print('!!!!')\n",
    "    for j in range(256):\n",
    "        if data_tsne_label[j]==i:\n",
    "           # print(i,str(i))\n",
    "           # pic[str(i)].append(data_tsne_rp[j])\n",
    "           # print(data_tsne_label[j])\n",
    "           print(data_tsne_rp[j])\n",
    "data_tsne_label=data_tsne_label.reshape((-1))\n",
    "'''\n",
    "#plt.hist(data_tsne_rp[:256], bins=200, range=(20,150),histtype=\"stepfilled\", alpha=.8)\n",
    "\n",
    "#print(np.histogram(data_tsne_rp[:200],bins=100,range=(0,100)))\n",
    "#for i in range(10):\n",
    "    #plt.hist(pic[str(i)], bins=100, range=(0,100),normed=True,histtype=\"stepfilled\", alpha=.8)\n",
    "    #print('!!!')\n",
    "    #print(pic[str(i)])\n",
    "#print(data_tsne_label)\n",
    "#data_tsne=np.array(data_tsne[:][])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/matplotlib/text.py:1191: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if HAS_SK:\n",
    "    # Visualization of trained flatten layer (T-SNE)\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "    plot_only = 255\n",
    "    \n",
    "    low_dim_embs = tsne.fit_transform(data_tsne_rp[:plot_only,:])\n",
    "    labels = data_tsne_label[:plot_only]\n",
    "    fig = plt.gcf()\n",
    "    plot_with_labels(low_dim_embs, labels)\n",
    "    \n",
    "    \n",
    "    fig.savefig('./checkpoint_cnn_multilayer2_layer_-2.png')\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "back_inform tensor(-0.0443, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.1629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "entroys [tensor(1.0168, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "informs [tensor(1.3647, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "costs [tensor(-75.1017, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.8131, device='cuda:0', grad_fn=<NegBackward>), tensor(0.7262, device='cuda:0', grad_fn=<NegBackward>), tensor(0.9025, device='cuda:0', grad_fn=<NegBackward>), tensor(1.3647, device='cuda:0', grad_fn=<NegBackward>), tensor(13.9892, device='cuda:0', grad_fn=<NegBackward>), tensor(-16.5019, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "entr tensor(2.6880, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(16.5019, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "infsums [tensor(-76.4664, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "back_inform tensor(-13.3667, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.1497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "entroys [tensor(0.9841, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "informs [tensor(1.2869, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "costs [tensor(-68.6733, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.7807, device='cuda:0', grad_fn=<NegBackward>), tensor(0.7234, device='cuda:0', grad_fn=<NegBackward>), tensor(0.8681, device='cuda:0', grad_fn=<NegBackward>), tensor(1.2869, device='cuda:0', grad_fn=<NegBackward>), tensor(14.4200, device='cuda:0', grad_fn=<NegBackward>), tensor(-16.3898, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "entr tensor(2.4535, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(16.3898, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "infsums [tensor(-69.9602, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "back_inform tensor(-14.8899, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.1721, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "entroys [tensor(0.9915, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "informs [tensor(1.2971, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "costs [tensor(-81.6580, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.8205, device='cuda:0', grad_fn=<NegBackward>), tensor(0.7332, device='cuda:0', grad_fn=<NegBackward>), tensor(0.8797, device='cuda:0', grad_fn=<NegBackward>), tensor(1.2971, device='cuda:0', grad_fn=<NegBackward>), tensor(14.3070, device='cuda:0', grad_fn=<NegBackward>), tensor(-16.3494, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "entr tensor(2.8129, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(16.3494, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "infsums [tensor(-82.9551, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "back_inform tensor(-16.3194, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.1837, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "entroys [tensor(1.0369, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "informs [tensor(1.3645, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "costs [tensor(-88.0392, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.8742, device='cuda:0', grad_fn=<NegBackward>), tensor(0.7448, device='cuda:0', grad_fn=<NegBackward>), tensor(0.9336, device='cuda:0', grad_fn=<NegBackward>), tensor(1.3645, device='cuda:0', grad_fn=<NegBackward>), tensor(15.7084, device='cuda:0', grad_fn=<NegBackward>), tensor(-17.2462, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "entr tensor(3.1689, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(17.2462, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "infsums [tensor(-89.4037, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "back_inform tensor(-16.1553, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.1749, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "entroys [tensor(0.9990, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "informs [tensor(1.3385, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "costs [tensor(-83.6823, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.8462, device='cuda:0', grad_fn=<NegBackward>), tensor(0.7354, device='cuda:0', grad_fn=<NegBackward>), tensor(0.9266, device='cuda:0', grad_fn=<NegBackward>), tensor(1.3385, device='cuda:0', grad_fn=<NegBackward>), tensor(14.9085, device='cuda:0', grad_fn=<NegBackward>), tensor(-17.1148, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "entr tensor(2.9936, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(17.1148, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "infsums [tensor(-85.0209, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "back_inform tensor(-15.7493, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.1674, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "entroys [tensor(0.9933, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "informs [tensor(1.3553, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "costs [tensor(-75.9227, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.7911, device='cuda:0', grad_fn=<NegBackward>), tensor(0.7215, device='cuda:0', grad_fn=<NegBackward>), tensor(0.8964, device='cuda:0', grad_fn=<NegBackward>), tensor(1.3553, device='cuda:0', grad_fn=<NegBackward>), tensor(14.2788, device='cuda:0', grad_fn=<NegBackward>), tensor(-16.3734, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "entr tensor(2.7404, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(16.3734, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "infsums [tensor(-77.2780, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "back_inform tensor(-15.7239, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.1659, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "entroys [tensor(1.0146, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "informs [tensor(1.3201, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "costs [tensor(-75.9590, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.8144, device='cuda:0', grad_fn=<NegBackward>), tensor(0.7323, device='cuda:0', grad_fn=<NegBackward>), tensor(0.8728, device='cuda:0', grad_fn=<NegBackward>), tensor(1.3201, device='cuda:0', grad_fn=<NegBackward>), tensor(14.4806, device='cuda:0', grad_fn=<NegBackward>), tensor(-16.1951, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "entr tensor(2.6870, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(16.1951, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "infsums [tensor(-77.2791, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "back_inform tensor(-16.3917, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.1658, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "entroys [tensor(1.0117, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "informs [tensor(1.4054, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "costs [tensor(-74.1243, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.7683, device='cuda:0', grad_fn=<NegBackward>), tensor(0.7106, device='cuda:0', grad_fn=<NegBackward>), tensor(0.8353, device='cuda:0', grad_fn=<NegBackward>), tensor(1.4054, device='cuda:0', grad_fn=<NegBackward>), tensor(14.8651, device='cuda:0', grad_fn=<NegBackward>), tensor(-16.7562, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "entr tensor(2.7780, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(16.7562, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "infsums [tensor(-75.5297, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "back_inform tensor(-17.2359, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.1675, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "entroys [tensor(0.9987, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "informs [tensor(1.2592, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "costs [tensor(-84.1458, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.8096, device='cuda:0', grad_fn=<NegBackward>), tensor(0.7264, device='cuda:0', grad_fn=<NegBackward>), tensor(0.8758, device='cuda:0', grad_fn=<NegBackward>), tensor(1.2592, device='cuda:0', grad_fn=<NegBackward>), tensor(14.8998, device='cuda:0', grad_fn=<NegBackward>), tensor(-17.6029, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "entr tensor(2.9487, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(17.6029, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "infsums [tensor(-85.4051, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "back_inform tensor(-16.8246, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.1637, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "entroys [tensor(1.0099, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "informs [tensor(1.2504, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "costs [tensor(-84.7207, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.8053, device='cuda:0', grad_fn=<NegBackward>), tensor(0.7258, device='cuda:0', grad_fn=<NegBackward>), tensor(0.8817, device='cuda:0', grad_fn=<NegBackward>), tensor(1.2504, device='cuda:0', grad_fn=<NegBackward>), tensor(15.4484, device='cuda:0', grad_fn=<NegBackward>), tensor(-17.0121, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "entr tensor(2.7855, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(17.0121, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "infsums [tensor(-85.9711, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "back_inform tensor(-16.7338, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.1580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "entroys [tensor(1.0141, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "informs [tensor(1.3554, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "costs [tensor(-77.2565, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.7996, device='cuda:0', grad_fn=<NegBackward>), tensor(0.7252, device='cuda:0', grad_fn=<NegBackward>), tensor(0.8489, device='cuda:0', grad_fn=<NegBackward>), tensor(1.3554, device='cuda:0', grad_fn=<NegBackward>), tensor(14.6982, device='cuda:0', grad_fn=<NegBackward>), tensor(-16.9235, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "entr tensor(2.6746, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(16.9235, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "infsums [tensor(-78.6119, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "back_inform tensor(-16.9420, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.1633, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "entroys [tensor(0.9947, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "informs [tensor(1.3211, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "costs [tensor(-84.1356, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.7938, device='cuda:0', grad_fn=<NegBackward>), tensor(0.7235, device='cuda:0', grad_fn=<NegBackward>), tensor(0.8717, device='cuda:0', grad_fn=<NegBackward>), tensor(1.3211, device='cuda:0', grad_fn=<NegBackward>), tensor(14.9320, device='cuda:0', grad_fn=<NegBackward>), tensor(-17.1879, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "entr tensor(2.8074, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(17.1879, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "infsums [tensor(-85.4567, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "back_inform tensor(-16.5502, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.1848, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "entroys [tensor(1.0419, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "informs [tensor(1.4514, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "costs [tensor(-78.4036, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.8126, device='cuda:0', grad_fn=<NegBackward>), tensor(0.7248, device='cuda:0', grad_fn=<NegBackward>), tensor(0.9003, device='cuda:0', grad_fn=<NegBackward>), tensor(1.4514, device='cuda:0', grad_fn=<NegBackward>), tensor(14.7240, device='cuda:0', grad_fn=<NegBackward>), tensor(-16.7680, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "entr tensor(3.0995, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(16.7680, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "infsums [tensor(-79.8550, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "back_inform tensor(-17.0252, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.1886, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "entroys [tensor(1.0212, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "informs [tensor(1.3552, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "costs [tensor(-86.0158, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.8629, device='cuda:0', grad_fn=<NegBackward>), tensor(0.7357, device='cuda:0', grad_fn=<NegBackward>), tensor(0.9074, device='cuda:0', grad_fn=<NegBackward>), tensor(1.3552, device='cuda:0', grad_fn=<NegBackward>), tensor(15.5685, device='cuda:0', grad_fn=<NegBackward>), tensor(-17.1216, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "entr tensor(3.2289, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(17.1216, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "infsums [tensor(-87.3709, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "back_inform tensor(-15.6962, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.1627, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "200\n",
      "entroys [tensor(0.9873, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "informs [tensor(1.2378, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "costs [tensor(-69.8911, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.7760, device='cuda:0', grad_fn=<NegBackward>), tensor(0.7188, device='cuda:0', grad_fn=<NegBackward>), tensor(0.8887, device='cuda:0', grad_fn=<NegBackward>), tensor(1.2378, device='cuda:0', grad_fn=<NegBackward>), tensor(14.1127, device='cuda:0', grad_fn=<NegBackward>), tensor(-15.8399, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "entr tensor(2.5775, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(15.8399, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "infsums [tensor(-71.1289, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "back_inform tensor(-16.9782, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.1765, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0\n",
      "entroys [tensor(1.0133, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "informs [tensor(1.3434, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "costs [tensor(-86.1153, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.8556, device='cuda:0', grad_fn=<NegBackward>), tensor(0.7354, device='cuda:0', grad_fn=<NegBackward>), tensor(0.9246, device='cuda:0', grad_fn=<NegBackward>), tensor(1.3434, device='cuda:0', grad_fn=<NegBackward>), tensor(15.2207, device='cuda:0', grad_fn=<NegBackward>), tensor(-17.0559, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "entr tensor(3.0104, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(17.0559, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "infsums [tensor(-87.4588, device='cuda:0', grad_fn=<SubBackward0>)]\n",
      "back_inform tensor(-16.3298, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(0.1780, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "100\n",
      "entroys [tensor(0.9936, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "informs [tensor(1.2480, device='cuda:0', grad_fn=<MulBackward0>)]\n",
      "costs [tensor(-83.7999, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.8191, device='cuda:0', grad_fn=<NegBackward>), tensor(0.7295, device='cuda:0', grad_fn=<NegBackward>), tensor(0.8823, device='cuda:0', grad_fn=<NegBackward>), tensor(1.2480, device='cuda:0', grad_fn=<NegBackward>), tensor(14.4437, device='cuda:0', grad_fn=<NegBackward>), tensor(-16.4689, device='cuda:0', grad_fn=<NegBackward>)]\n",
      "entr tensor(2.9308, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "inf tensor(16.4689, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "infsums [tensor(-85.0478, device='cuda:0', grad_fn=<SubBackward0>)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-78ca771695e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0minformation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlamda\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0memerinal_hsic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mentropy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memerinal_hsic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0minformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-78ca771695e6>\u001b[0m in \u001b[0;36memerinal_hsic\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mKx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgram_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mKy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgram_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-78ca771695e6>\u001b[0m in \u001b[0;36mgram_matrix\u001b[0;34m(data, kernel)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mbuf3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdatav1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#print(buf3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbuf3\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0memerinal_hsic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "delta=5\n",
    "def gausin_distance(x,y,delta=delta):\n",
    "    H= torch.norm(x-y).cuda()\n",
    "    distance = torch.exp(-H/2/(delta**2)).cuda()\n",
    "    \n",
    "    return distance\n",
    "def gram_matrix(data,kernel=gausin_distance):\n",
    "    #num=data.shape[0]\n",
    "    #matrix=torch.zeros((num,num)).cuda()\n",
    "    #for i in range (0,num):\n",
    "    #   for j in range (0,num):\n",
    "    #        matrix[i][j]=kernel(data[i],data[j])\n",
    "    num=data.shape[0]\n",
    "    datav3=torch.mm(data,torch.transpose(data, 0, 1).cuda()).cuda()\n",
    "    datav1=torch.diag(datav3,0).cuda()\n",
    "    #print(datav1.shape)\n",
    "    #matrix=torch.sqrt(datav1+datav2-2*datav3)\n",
    "    buf1=((-2)*datav3+datav1).cuda()\n",
    "    buf2=torch.transpose(buf1, 0, 1).cuda()\n",
    "    buf3=(buf2+datav1).cuda()\n",
    "    #print(buf3)\n",
    "    matrix = torch.exp(-buf3/2/(delta**2)).cuda()\n",
    "    return matrix\n",
    "def emerinal_hsic(X,Y):\n",
    "    num=X.shape[0]\n",
    "    #print(num)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Kx=gram_matrix(X)\n",
    "    \n",
    "    Ky=gram_matrix(Y)\n",
    "    \n",
    "    H = torch.eye(num).cuda() - torch.ones((num,num), dtype = torch.float32).cuda() / num\n",
    "    #print(H)\n",
    "    hsic=1/(num-1)*torch.trace(torch.mm(torch.mm(torch.mm(Kx,H).cuda(),Ky).cuda(),H).cuda()).cuda()\n",
    "    \n",
    "    return hsic\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets\n",
    "from PIL import Image\n",
    "batch_size=256\n",
    "class MyMNIST(torchvision.datasets.MNIST):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None,download=False):\n",
    "        \n",
    "        super(MyMNIST,self).__init__(root, train, transform, target_transform,download)\n",
    "        #print(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "        #print(target) \n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "            \n",
    "        else:\n",
    "            target=torch.as_tensor(target)\n",
    "            target = F.one_hot(target,num_classes=10).cuda()\n",
    "\n",
    "        return img, target\n",
    "mnist_data=torchvision.datasets.MNIST(root='./MNIST/', train=True, transform=transforms.ToTensor(), target_transform=None, download=False)\n",
    "data_loader = torch.utils.data.DataLoader(mnist_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2)\n",
    "#D_in, H1,H2,H3,H4,H5,H6,H7, D_out = 784, 64,32,16,8,4,2,1,10\n",
    "layer_num=[784,64]\n",
    "costs=[]\n",
    "dtype=torch.float\n",
    "continue_flag=1\n",
    "PATH='./checkpoint_cnn_multilayer2'\n",
    "w=[]\n",
    "z=[]\n",
    "rate_list=[]\n",
    "if continue_flag:\n",
    "    \n",
    "    checkpoint = torch.load(PATH)\n",
    "    \n",
    "    w=checkpoint['w']\n",
    "\n",
    "else:\n",
    "    for i in range(len(layer_num)-1):\n",
    "    \n",
    "        \n",
    "        m = torch.nn.Conv2d(1, 32, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(32, 64, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(64, 32, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(32, 16, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(16, 3, 3, stride=1,padding=2).cuda()\n",
    "        w.append(m)\n",
    "        m=torch.nn.Conv2d(3, 1, 2, stride=1,padding=0).cuda()\n",
    "        w.append(m)\n",
    "        \n",
    "\n",
    "epoches=20\n",
    "lamda=20\n",
    "#5000\n",
    "learning_rate=0\n",
    "learning_rate_post=0.01\n",
    "data_tsne_label=[]\n",
    "data_tsne_rp1=[]\n",
    "data_tsne_rp2=[]\n",
    "linear=torch.nn.Linear(144,1).cuda()\n",
    "for i in range(epoches): \n",
    "    for i_batch, (img,target) in enumerate(data_loader):\n",
    "        z=[]\n",
    "        costs=[]\n",
    "        img=img.reshape(-1,1,28,28).cuda()\n",
    "        \n",
    "        target=torch.as_tensor(target).cuda()\n",
    "        num=target.shape[0]\n",
    "        #print(num)\n",
    "        target=target.reshape(num,1).cuda()\n",
    "        #target = F.one_hot(target,num_classes=10).cuda()\n",
    "        target=torch.zeros((num,10)).cuda().scatter_(1,target,1).cuda()\n",
    "        \n",
    "        target=torch.as_tensor(target,dtype=dtype).cuda()\n",
    "        \n",
    "        z.append(F.relu(F.max_pool2d(w[0](img), 2)).cuda())\n",
    "        #print(z[0].shape)\n",
    "        #for cnt in range(len(w)-1):\n",
    "            \n",
    "        #   z.append(torch.tanh(torch.mm(z[cnt],w[cnt+1]).cuda()).cuda())\n",
    "        z.append(F.relu(F.max_pool2d(w[1](z[0]), 2)).cuda())\n",
    "        #print(z[1].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[2](z[1]), 2)).cuda())\n",
    "        #print(z[2].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[3](z[2]), 2)).cuda())\n",
    "        #print(z[3].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[4](z[3]), 2)).cuda())\n",
    "        #print(z[4].shape)\n",
    "        z.append(F.relu(F.max_pool2d(w[5](z[4]), 2)).cuda())\n",
    "        #print(z[5].shape)\n",
    "        entropys=[]\n",
    "        informs=[]\n",
    "        infsums=[]\n",
    "        #print(z[1].shape)\n",
    "        h=z[3].view(img.shape[0],-1).cuda()\n",
    "        post=linear(h)\n",
    "        \n",
    "        #print(h[:,1].shape)\n",
    "        information=lamda*emerinal_hsic(h.view(img.shape[0],-1).cuda(),target)\n",
    "        entropy=emerinal_hsic(h.view(img.shape[0],-1).cuda(),img.view(-1,784).cuda())\n",
    "        \n",
    "        back_inform=-lamda*emerinal_hsic(post.view(img.shape[0],-1).cuda(),target)\n",
    "        \n",
    "        cost=0\n",
    "        number=0\n",
    "        for i in range(h.shape[1]):\n",
    "            cost=cost-lamda*emerinal_hsic(h[:,i].view(-1,1),target)\n",
    "            xnova=lamda*emerinal_hsic(h[:,i].view(-1,1),target)\n",
    "            #if xnova > 3:\n",
    "                #print(i)\n",
    "        #print('!!!!!')\n",
    "        #print(lamda*emerinal_hsic(h.view(256,-1),target))\n",
    "        infsum=cost\n",
    "        cost=cost+information\n",
    "        entropys.append(entropy)\n",
    "        informs.append(information)\n",
    "        infsums.append(infsum)\n",
    "        \n",
    "        costs.append(cost)\n",
    "        \n",
    "        grad_list=[]\n",
    "        \n",
    "        for cnt in range(len(z)-1):\n",
    "            information=-lamda*emerinal_hsic(z[cnt].view(img.shape[0],-1).cuda(),target)\n",
    "            entropy=emerinal_hsic(z[cnt].view(img.shape[0],-1).cuda(),img.view(-1,784).cuda())\n",
    "            cost=-information\n",
    "            costs.append(cost)\n",
    "        #print(cost1)\n",
    "        #    cost.backward(retain_graph=True)\n",
    "        #    print('!')\n",
    "        #print(w1.cuda().grad)\n",
    "        #   with torch.no_grad():\n",
    "        #        buf=torch.zeros((w[cnt].shape)).cuda()\n",
    "                #print(buf.shape)\n",
    "        #       buf.copy_(w[cnt].grad)\n",
    "                \n",
    "        #        grad_list.append(buf)\n",
    "            #print('grad1',torch.norm(grad1))\n",
    "            #w2 -= learning_rate * w2.grad\n",
    "\n",
    "            # Manually zero the gradients after updating weights\n",
    "        #        w[cnt].grad.zero_()\n",
    "                #print(cnt,torch.norm(buf))\n",
    "            #w2.grad.zero_()\n",
    "        z[-1]=z[-1].view(img.shape[0],-1).cuda()\n",
    "        #print(z[-1].shape)\n",
    "        img=img.view(img.shape[0],-1).cuda()\n",
    "        entropy=emerinal_hsic(z[-1],img)\n",
    "        information=lamda*emerinal_hsic(z[-1],target)\n",
    "        rate=entropy/information\n",
    "        cost=-information\n",
    "        costs.append(cost)\n",
    "        #+0.00005*(entropy/information)\n",
    "        #cost.backward()\n",
    "        back_inform.backward()\n",
    "        with torch.no_grad():\n",
    "            #w1 -= learning_rate * w1.grad\n",
    "            #print('grad2',torch.norm(w2.grad))\n",
    "            #w[-1].weight -= learning_rate * w[-1].weight.grad\n",
    "            \n",
    "            \n",
    "            #w[0].weight -= learning_rate * w[0].weight.grad\n",
    "            #w[1].weight -= learning_rate * w[1].weight.grad\n",
    "            #w[2].weight -= learning_rate * w[2].weight.grad\n",
    "            #w[3].weight -= learning_rate * w[3].weight.grad\n",
    "            #w[4].weight -= learning_rate * w[4].weight.grad\n",
    "            linear.weight -=learning_rate_post*linear.weight.grad\n",
    "            linear.bias -=learning_rate_post*linear.bias.grad\n",
    "        # Manually zero the gradients after updating weights\n",
    "            #w[-1].weight.grad.zero_()\n",
    "            #w[0].weight.grad.zero_()\n",
    "            #w[1].weight.grad.zero_()\n",
    "            #w[2].weight.grad.zero_()\n",
    "            #w[3].weight.grad.zero_()\n",
    "            #w[4].weight.grad.zero_()\n",
    "            linear.weight.grad.zero_()\n",
    "            linear.bias.grad.zero_()\n",
    "        #    data_tsne_rp2.append(z2.cpu().data.numpy())\n",
    "        #    data_tsne_rp1.append(z1.cpu().data.numpy())\n",
    "        #    data_tsne_label.append(target.argmax(dim=1, keepdim=True).cpu().data.numpy())\n",
    "        \n",
    "\n",
    "        if i_batch %100 ==0:\n",
    "            #costs.append(cost)\n",
    "            print('back_inform',back_inform)\n",
    "            print(rate)\n",
    "            rate_list.append(rate)\n",
    "            print(i_batch)\n",
    "            print('entroys',entropys)\n",
    "            print('informs',informs)\n",
    "            print('costs',costs)\n",
    "            print('entr',entropy)\n",
    "            print('inf',information)\n",
    "            print('infsums',infsums)\n",
    "            data_tsne_label.append(target.argmax(dim=1, keepdim=True).cpu().data.numpy())\n",
    "            torch.save({\n",
    "            'w': w,\n",
    "            'z': z,\n",
    "            'data_tsne_label':target.argmax(dim=1, keepdim=True).cpu().data.numpy()\n",
    "            ,'costs':costs\n",
    "            #'data_tsne_rp2':data_tsne_rp2,\n",
    "            #'data_tsne_rp1':data_tsne_rp1\n",
    "                \n",
    "            }, PATH)\n",
    "            \n",
    "            data_tsne_label=[]\n",
    "            #data_tsne_rp1=[]\n",
    "            #data_tsne_rp2=[]\n",
    "\n",
    "            #for j,zz in enumerate(z2):\n",
    "                #print(j,zz)\n",
    "                #print(j,target[j])\n",
    "            \n",
    "plt.plot(rate_list)\n",
    "plt.ylabel('rate')\n",
    "plt.xlabel('epochs (per 1)')\n",
    "plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
